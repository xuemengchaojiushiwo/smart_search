#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶åˆ‡å—æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸åŒæ–‡ä»¶ç±»å‹çš„åˆ‡å—æ•ˆæœå’Œæ€§èƒ½
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import List, Dict, Any
import tempfile

# æ·»åŠ python_serviceç›®å½•åˆ°è·¯å¾„
sys.path.append('python_service')

# å¯¼å…¥æ–‡æ¡£å¤„ç†ç›¸å…³åº“
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# å¯¼å…¥æ–‡æ¡£å¤„ç†åº“
from docx import Document as DocxDocument
from openpyxl import load_workbook
from pptx import Presentation

# å¯¼å…¥é…ç½®
from python_service.config import DOCUMENT_CONFIG, CHUNKING_CONFIG

class ChunkingTester:
    def __init__(self):
        self.results = {}
        self.test_files = {
            'pdf': ['python_service/file/å®‰è”ç¾å…ƒ.pdf', 'python_service/file/åº—é“ºå…¥ä½æµç¨‹.pdf'],
            'docx': ['python_service/file/manusä»‹ç».docx'],
            'xlsx': ['python_service/file/å°çº¢ä¹¦é€‰å“.xlsx'],
            'pptx': ['python_service/file/network_skill.pptx']
        }
        
    def test_pdf_chunking(self, file_path: str) -> Dict[str, Any]:
        """æµ‹è¯•PDFæ–‡ä»¶åˆ‡å—"""
        print(f"æµ‹è¯•PDFæ–‡ä»¶: {file_path}")
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨PyMuPDFLoaderå¤„ç†PDF
            loader = PyMuPDFLoader(file_path)
            documents = loader.load()
            
            # æ–‡æœ¬åˆ†å‰²
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=DOCUMENT_CONFIG['chunk_size'],
                chunk_overlap=DOCUMENT_CONFIG['chunk_overlap'],
                length_function=len,
            )
            chunks = text_splitter.split_documents(documents)
            
            end_time = time.time()
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_text_length = sum(len(chunk.page_content) for chunk in chunks)
            avg_chunk_length = total_text_length / len(chunks) if chunks else 0
            
            # åˆ†æchunkå†…å®¹
            chunk_analysis = []
            for i, chunk in enumerate(chunks[:5]):  # åªåˆ†æå‰5ä¸ªchunk
                chunk_analysis.append({
                    'chunk_index': i,
                    'length': len(chunk.page_content),
                    'preview': chunk.page_content[:100] + '...' if len(chunk.page_content) > 100 else chunk.page_content,
                    'metadata': chunk.metadata
                })
            
            return {
                'success': True,
                'processing_time': end_time - start_time,
                'total_chunks': len(chunks),
                'total_text_length': total_text_length,
                'avg_chunk_length': avg_chunk_length,
                'chunk_analysis': chunk_analysis,
                'file_size_mb': os.path.getsize(file_path) / (1024 * 1024)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def test_docx_chunking(self, file_path: str) -> Dict[str, Any]:
        """æµ‹è¯•DOCXæ–‡ä»¶åˆ‡å—"""
        print(f"æµ‹è¯•DOCXæ–‡ä»¶: {file_path}")
        
        start_time = time.time()
        
        try:
            # å¤„ç†Wordæ–‡æ¡£
            doc = DocxDocument(file_path)
            text_parts = []
            
            # æå–æ®µè½æ–‡æœ¬
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_parts.append(paragraph.text)
            
            # æå–è¡¨æ ¼æ–‡æœ¬
            for table in doc.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        text_parts.append(" | ".join(row_text))
            
            text = "\n".join(text_parts)
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=DOCUMENT_CONFIG['chunk_size'],
                chunk_overlap=DOCUMENT_CONFIG['chunk_overlap'],
                length_function=len,
            )
            chunks = text_splitter.split_text(text)
            chunks = [Document(page_content=chunk) for chunk in chunks]
            
            end_time = time.time()
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_text_length = sum(len(chunk.page_content) for chunk in chunks)
            avg_chunk_length = total_text_length / len(chunks) if chunks else 0
            
            # åˆ†æchunkå†…å®¹
            chunk_analysis = []
            for i, chunk in enumerate(chunks[:5]):  # åªåˆ†æå‰5ä¸ªchunk
                chunk_analysis.append({
                    'chunk_index': i,
                    'length': len(chunk.page_content),
                    'preview': chunk.page_content[:100] + '...' if len(chunk.page_content) > 100 else chunk.page_content
                })
            
            return {
                'success': True,
                'processing_time': end_time - start_time,
                'total_chunks': len(chunks),
                'total_text_length': total_text_length,
                'avg_chunk_length': avg_chunk_length,
                'chunk_analysis': chunk_analysis,
                'file_size_mb': os.path.getsize(file_path) / (1024 * 1024)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def test_xlsx_chunking(self, file_path: str) -> Dict[str, Any]:
        """æµ‹è¯•XLSXæ–‡ä»¶åˆ‡å—"""
        print(f"æµ‹è¯•XLSXæ–‡ä»¶: {file_path}")
        
        start_time = time.time()
        
        try:
            # å¤„ç†Excelæ–‡ä»¶
            workbook = load_workbook(file_path, data_only=True)
            text_parts = []
            
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                text_parts.append(f"å·¥ä½œè¡¨: {sheet_name}")
                
                for row in sheet.iter_rows(values_only=True):
                    row_text = []
                    for cell_value in row:
                        if cell_value is not None and str(cell_value).strip():
                            row_text.append(str(cell_value).strip())
                    if row_text:
                        text_parts.append(" | ".join(row_text))
            
            text = "\n".join(text_parts)
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=DOCUMENT_CONFIG['chunk_size'],
                chunk_overlap=DOCUMENT_CONFIG['chunk_overlap'],
                length_function=len,
            )
            chunks = text_splitter.split_text(text)
            chunks = [Document(page_content=chunk) for chunk in chunks]
            
            end_time = time.time()
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_text_length = sum(len(chunk.page_content) for chunk in chunks)
            avg_chunk_length = total_text_length / len(chunks) if chunks else 0
            
            # åˆ†æchunkå†…å®¹
            chunk_analysis = []
            for i, chunk in enumerate(chunks[:5]):  # åªåˆ†æå‰5ä¸ªchunk
                chunk_analysis.append({
                    'chunk_index': i,
                    'length': len(chunk.page_content),
                    'preview': chunk.page_content[:100] + '...' if len(chunk.page_content) > 100 else chunk.page_content
                })
            
            return {
                'success': True,
                'processing_time': end_time - start_time,
                'total_chunks': len(chunks),
                'total_text_length': total_text_length,
                'avg_chunk_length': avg_chunk_length,
                'chunk_analysis': chunk_analysis,
                'file_size_mb': os.path.getsize(file_path) / (1024 * 1024)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def test_pptx_chunking(self, file_path: str) -> Dict[str, Any]:
        """æµ‹è¯•PPTXæ–‡ä»¶åˆ‡å—"""
        print(f"æµ‹è¯•PPTXæ–‡ä»¶: {file_path}")
        
        start_time = time.time()
        
        try:
            # å¤„ç†PowerPointæ–‡ä»¶
            prs = Presentation(file_path)
            text_parts = []
            
            for slide_number, slide in enumerate(prs.slides, 1):
                text_parts.append(f"å¹»ç¯ç‰‡ {slide_number}:")
                
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        text_parts.append(shape.text.strip())
                
                text_parts.append("")  # ç©ºè¡Œåˆ†éš”å¹»ç¯ç‰‡
            
            text = "\n".join(text_parts)
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=DOCUMENT_CONFIG['chunk_size'],
                chunk_overlap=DOCUMENT_CONFIG['chunk_overlap'],
                length_function=len,
            )
            chunks = text_splitter.split_text(text)
            chunks = [Document(page_content=chunk) for chunk in chunks]
            
            end_time = time.time()
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_text_length = sum(len(chunk.page_content) for chunk in chunks)
            avg_chunk_length = total_text_length / len(chunks) if chunks else 0
            
            # åˆ†æchunkå†…å®¹
            chunk_analysis = []
            for i, chunk in enumerate(chunks[:5]):  # åªåˆ†æå‰5ä¸ªchunk
                chunk_analysis.append({
                    'chunk_index': i,
                    'length': len(chunk.page_content),
                    'preview': chunk.page_content[:100] + '...' if len(chunk.page_content) > 100 else chunk.page_content
                })
            
            return {
                'success': True,
                'processing_time': end_time - start_time,
                'total_chunks': len(chunks),
                'total_text_length': total_text_length,
                'avg_chunk_length': avg_chunk_length,
                'chunk_analysis': chunk_analysis,
                'file_size_mb': os.path.getsize(file_path) / (1024 * 1024)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=" * 60)
        print("å¼€å§‹æ–‡ä»¶åˆ‡å—æµ‹è¯•")
        print("=" * 60)
        
        # æµ‹è¯•PDFæ–‡ä»¶
        for pdf_file in self.test_files['pdf']:
            if os.path.exists(pdf_file):
                result = self.test_pdf_chunking(pdf_file)
                self.results[f"PDF_{Path(pdf_file).name}"] = result
                self.print_result("PDF", Path(pdf_file).name, result)
        
        # æµ‹è¯•DOCXæ–‡ä»¶
        for docx_file in self.test_files['docx']:
            if os.path.exists(docx_file):
                result = self.test_docx_chunking(docx_file)
                self.results[f"DOCX_{Path(docx_file).name}"] = result
                self.print_result("DOCX", Path(docx_file).name, result)
        
        # æµ‹è¯•XLSXæ–‡ä»¶
        for xlsx_file in self.test_files['xlsx']:
            if os.path.exists(xlsx_file):
                result = self.test_xlsx_chunking(xlsx_file)
                self.results[f"XLSX_{Path(xlsx_file).name}"] = result
                self.print_result("XLSX", Path(xlsx_file).name, result)
        
        # æµ‹è¯•PPTXæ–‡ä»¶
        for pptx_file in self.test_files['pptx']:
            if os.path.exists(pptx_file):
                result = self.test_pptx_chunking(pptx_file)
                self.results[f"PPTX_{Path(pptx_file).name}"] = result
                self.print_result("PPTX", Path(pptx_file).name, result)
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_report()
    
    def print_result(self, file_type: str, filename: str, result: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print(f"\n{'-' * 40}")
        print(f"æ–‡ä»¶ç±»å‹: {file_type}")
        print(f"æ–‡ä»¶å: {filename}")
        
        if result['success']:
            print(f"âœ… å¤„ç†æˆåŠŸ")
            print(f"ğŸ“Š å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
            print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {result['file_size_mb']:.2f}MB")
            print(f"ğŸ”¢ æ€»å—æ•°: {result['total_chunks']}")
            print(f"ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: {result['total_text_length']}å­—ç¬¦")
            print(f"ğŸ“ å¹³å‡å—é•¿åº¦: {result['avg_chunk_length']:.1f}å­—ç¬¦")
            
            if result['chunk_analysis']:
                print(f"\nğŸ“‹ å‰5ä¸ªå—åˆ†æ:")
                for chunk_info in result['chunk_analysis']:
                    print(f"  å—{chunk_info['chunk_index']}: {chunk_info['length']}å­—ç¬¦")
                    print(f"    é¢„è§ˆ: {chunk_info['preview']}")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥")
            print(f"ğŸš¨ é”™è¯¯ä¿¡æ¯: {result['error']}")
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print(f"\n{'=' * 60}")
        print("æµ‹è¯•æŠ¥å‘Šæ€»ç»“")
        print(f"{'=' * 60}")
        
        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
        success_count = sum(1 for result in self.results.values() if result['success'])
        total_count = len(self.results)
        
        print(f"ğŸ“ˆ æ€»æµ‹è¯•æ–‡ä»¶æ•°: {total_count}")
        print(f"âœ… æˆåŠŸå¤„ç†: {success_count}")
        print(f"âŒ å¤±è´¥å¤„ç†: {total_count - success_count}")
        
        if success_count > 0:
            # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
            avg_time = sum(result['processing_time'] for result in self.results.values() if result['success']) / success_count
            print(f"â±ï¸ å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ç§’")
            
            # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
            type_stats = {}
            for key, result in self.results.items():
                if result['success']:
                    file_type = key.split('_')[0]
                    if file_type not in type_stats:
                        type_stats[file_type] = []
                    type_stats[file_type].append(result)
            
            print(f"\nğŸ“Š æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡:")
            for file_type, results in type_stats.items():
                avg_chunks = sum(r['total_chunks'] for r in results) / len(results)
                avg_length = sum(r['avg_chunk_length'] for r in results) / len(results)
                print(f"  {file_type}: å¹³å‡{avg_chunks:.1f}å—, å¹³å‡å—é•¿åº¦{avg_length:.1f}å­—ç¬¦")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = "chunking_test_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    tester = ChunkingTester()
    tester.run_all_tests()

if __name__ == "__main__":
    main() 