#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„æœç´¢ç­–ç•¥
å¤„ç†å¤šæ–‡æ¡£åœºæ™¯ä¸‹çš„ç²¾ç¡®åŒ¹é…é—®é¢˜
"""

import re
from typing import List, Dict, Optional

def extract_fund_names_from_question(question: str) -> List[str]:
    """
    ä»é—®é¢˜ä¸­æå–åŸºé‡‘åç§°
    """
    # å¸¸è§çš„åŸºé‡‘åç§°æ¨¡å¼
    fund_patterns = [
        r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]+åŸºé‡‘)',  # åŒ¹é…"XXåŸºé‡‘"
        r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]+ç¾å…ƒ)',  # åŒ¹é…"XXç¾å…ƒ"
        r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]+å€ºåˆ¸)',  # åŒ¹é…"XXå€ºåˆ¸"
        r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]+è‚¡ç¥¨)',  # åŒ¹é…"XXè‚¡ç¥¨"
    ]
    
    fund_names = []
    for pattern in fund_patterns:
        matches = re.findall(pattern, question)
        fund_names.extend(matches)
    
    # å»é‡å¹¶è¿‡æ»¤
    fund_names = list(set(fund_names))
    fund_names = [name for name in fund_names if len(name) > 2]  # è¿‡æ»¤å¤ªçŸ­çš„
    
    return fund_names

def classify_question_type(question: str) -> str:
    """
    åˆ†ç±»é—®é¢˜ç±»å‹
    """
    question_lower = question.lower()
    
    if any(word in question_lower for word in ['æ€»å€¼', 'è§„æ¨¡', 'èµ„äº§', 'é‡‘é¢']):
        return "fund_size"
    elif any(word in question_lower for word in ['æŠ•èµ„ç›®æ ‡', 'ç›®æ ‡', 'ç­–ç•¥']):
        return "investment_objective"
    elif any(word in question_lower for word in ['åŸºé‡‘ç»ç†', 'ç»ç†', 'ç®¡ç†äºº']):
        return "fund_manager"
    elif any(word in question_lower for word in ['ç®¡ç†è´¹', 'è´¹ç”¨', 'è´¹ç‡']):
        return "management_fee"
    elif any(word in question_lower for word in ['æˆç«‹æ—¥æœŸ', 'æˆç«‹', 'æˆç«‹æ—¶é—´']):
        return "establishment_date"
    elif any(word in question_lower for word in ['æ”¶ç›Šç‡', 'å›æŠ¥', 'æ”¶ç›Š']):
        return "performance"
    else:
        return "general"

def enhanced_chunk_filtering(question: str, chunks: List[Dict]) -> List[Dict]:
    """
    å¢å¼ºçš„chunkè¿‡æ»¤ç­–ç•¥
    """
    # 1. æå–é—®é¢˜ä¸­çš„å…³é”®ä¿¡æ¯
    fund_names = extract_fund_names_from_question(question)
    question_type = classify_question_type(question)
    
    print(f"ğŸ” é—®é¢˜åˆ†æ:")
    print(f"   åŸºé‡‘åç§°: {fund_names}")
    print(f"   é—®é¢˜ç±»å‹: {question_type}")
    
    # 2. å¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®šåŸºé‡‘åç§°ï¼Œè¿”å›æ‰€æœ‰chunks
    if not fund_names:
        print("   æœªæ£€æµ‹åˆ°å…·ä½“åŸºé‡‘åç§°ï¼Œè¿”å›æ‰€æœ‰chunks")
        return chunks
    
    # 3. æŒ‰åŸºé‡‘åç§°è¿‡æ»¤chunks
    filtered_chunks = []
    for chunk in chunks:
        chunk_content = chunk.get('content', '').lower()
        chunk_metadata = chunk.get('metadata', {})
        
        # æ£€æŸ¥chunkå†…å®¹æˆ–å…ƒæ•°æ®ä¸­æ˜¯å¦åŒ…å«åŸºé‡‘åç§°
        is_relevant = False
        
        # æ–¹æ³•1: æ£€æŸ¥chunkå†…å®¹
        for fund_name in fund_names:
            if fund_name.lower() in chunk_content:
                is_relevant = True
                break
        
        # æ–¹æ³•2: æ£€æŸ¥å…ƒæ•°æ®ä¸­çš„åŸºé‡‘åç§°
        if not is_relevant:
            chunk_fund_name = chunk_metadata.get('fund_name', '')
            if chunk_fund_name and any(fund_name.lower() in chunk_fund_name.lower() for fund_name in fund_names):
                is_relevant = True
        
        # æ–¹æ³•3: æ£€æŸ¥æ–‡ä»¶åï¼ˆä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
        if not is_relevant:
            source_file = chunk_metadata.get('source_file', '')
            if source_file and any(fund_name.lower() in source_file.lower() for fund_name in fund_names):
                is_relevant = True
        
        if is_relevant:
            filtered_chunks.append(chunk)
    
    print(f"   è¿‡æ»¤å‰chunksæ•°é‡: {len(chunks)}")
    print(f"   è¿‡æ»¤åchunksæ•°é‡: {len(filtered_chunks)}")
    
    # 4. å¦‚æœè¿‡æ»¤åchunkså¤ªå°‘ï¼Œæ”¾å®½æ¡ä»¶
    if len(filtered_chunks) < 3:
        print("   âš ï¸ è¿‡æ»¤åchunkså¤ªå°‘ï¼Œæ”¾å®½è¿‡æ»¤æ¡ä»¶")
        # ä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…ï¼šåªè¦åŒ…å«éƒ¨åˆ†å…³é”®è¯å³å¯
        for chunk in chunks:
            if chunk not in filtered_chunks:
                chunk_content = chunk.get('content', '').lower()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºé‡‘åç§°çš„éƒ¨åˆ†å…³é”®è¯
                for fund_name in fund_names:
                    fund_keywords = fund_name.lower().split()
                    if any(keyword in chunk_content for keyword in fund_keywords if len(keyword) > 1):
                        filtered_chunks.append(chunk)
                        break
    
    print(f"   æœ€ç»ˆchunksæ•°é‡: {len(filtered_chunks)}")
    return filtered_chunks

def smart_search_strategy(question: str, all_chunks: List[Dict]) -> List[Dict]:
    """
    æ™ºèƒ½æœç´¢ç­–ç•¥
    """
    print(f"\nğŸ§  æ™ºèƒ½æœç´¢ç­–ç•¥å¯åŠ¨")
    print(f"é—®é¢˜: {question}")
    
    # 1. å¢å¼ºè¿‡æ»¤
    filtered_chunks = enhanced_chunk_filtering(question, all_chunks)
    
    # 2. å¦‚æœè¿‡æ»¤åchunksä»ç„¶å¤ªå°‘ï¼Œè¿”å›æ‰€æœ‰chunks
    if len(filtered_chunks) < 2:
        print("   âš ï¸ è¿‡æ»¤åchunksä»ç„¶å¤ªå°‘ï¼Œè¿”å›æ‰€æœ‰chunks")
        return all_chunks
    
    return filtered_chunks

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¨¡æ‹Ÿchunksæ•°æ®
    sample_chunks = [
        {
            "content": "å®‰è”ç¾å…ƒé«˜æ”¶ç›ŠåŸºé‡‘çš„æ€»å€¼ä¸º4.4377äº¿ç¾å…ƒ",
            "metadata": {
                "source_file": "å®‰è”ç¾å…ƒ.pdf",
                "fund_name": "å®‰è”ç¾å…ƒé«˜æ”¶ç›ŠåŸºé‡‘",
                "page_num": 1
            }
        },
        {
            "content": "åå¤æˆé•¿åŸºé‡‘çš„æŠ•èµ„ç›®æ ‡æ˜¯è¿½æ±‚é•¿æœŸèµ„æœ¬å¢å€¼",
            "metadata": {
                "source_file": "åå¤æˆé•¿.pdf",
            "fund_name": "åå¤æˆé•¿åŸºé‡‘",
                "page_num": 1
            }
        },
        {
            "content": "å®‰è”ç¾å…ƒåŸºé‡‘çš„åŸºé‡‘ç»ç†æ˜¯Justin Kass",
            "metadata": {
                "source_file": "å®‰è”ç¾å…ƒ.pdf",
                "fund_name": "å®‰è”ç¾å…ƒé«˜æ”¶ç›ŠåŸºé‡‘",
                "page_num": 1
            }
        }
    ]
    
    # æµ‹è¯•ä¸åŒé—®é¢˜
    test_questions = [
        "å®‰è”ç¾å…ƒåŸºé‡‘çš„æ€»å€¼æ˜¯å¤šå°‘ï¼Ÿ",
        "åå¤æˆé•¿åŸºé‡‘çš„æŠ•èµ„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ",
        "åŸºé‡‘ç»ç†æ˜¯è°ï¼Ÿ",  # æ²¡æœ‰æŒ‡å®šå…·ä½“åŸºé‡‘
        "åŸºé‡‘çš„æ€»å€¼æ˜¯å¤šå°‘ï¼Ÿ"  # æ²¡æœ‰æŒ‡å®šå…·ä½“åŸºé‡‘
    ]
    
    for question in test_questions:
        print(f"\n{'='*50}")
        result_chunks = smart_search_strategy(question, sample_chunks)
        print(f"æœç´¢ç»“æœ: {len(result_chunks)} ä¸ªchunks")
        for i, chunk in enumerate(result_chunks[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
            print(f"  {i+1}. {chunk['content'][:50]}...")
