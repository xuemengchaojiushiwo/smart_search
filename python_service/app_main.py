#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ - ä¸»åº”ç”¨
æ•´åˆPyMuPDF Pro + PyMuPDF4LLM + LangChain + æå®¢æ™ºåŠAPI
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import json
import logging
import os
import tempfile
from pathlib import Path
import subprocess
from shutil import which
import shutil
import requests

# PyMuPDFç›¸å…³
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("âŒ PyMuPDF ä¸å¯ç”¨")

# å¯é€‰ï¼šç”¨äºä¼˜åŒ–Excelåˆ†é¡µçš„é¢„å¤„ç†
try:
    from openpyxl import load_workbook
    from openpyxl.worksheet.properties import PageSetupProperties
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

# LangChainç›¸å…³
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.schema import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âŒ LangChain ä¸å¯ç”¨")

# ESç›¸å…³
from elasticsearch import Elasticsearch
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ", version="2.0.0")

# å¯¼å…¥é…ç½®
from config import (
    ES_CONFIG, DOCUMENT_CONFIG, EMBEDDING_CONFIG, RAG_CONFIG,
    CHUNKING_CONFIG, GEEKAI_API_KEY, GEEKAI_CHAT_URL,
    GEEKAI_EMBEDDING_URL, DEFAULT_EMBEDDING_MODEL
)
from fastapi.responses import FileResponse

# è¾…åŠ©ï¼šæ„å»ºé¡µæ–‡æœ¬ä¸è¯çº§ç´¢å¼•ï¼ˆç”¨äºbboxå®šä½ï¼‰
def build_page_text_and_word_index(page: "fitz.Page") -> (str, list):
    """è¿”å› (page_text, word_entries), å…¶ä¸­ word_entries ä¸º [ (start, end, (x0,y0,x1,y1)) ]."""
    try:
        words = page.get_text("words")  # (x0,y0,x1,y1,word,block_no,line_no,word_no)
        # æ’åºï¼šå—â†’è¡Œâ†’è¯
        words_sorted = sorted(words, key=lambda w: (int(w[5]), int(w[6]), int(w[7])))
        parts = []
        entries = []
        last_block, last_line = None, None
        current_pos = 0
        for w in words_sorted:
            x0, y0, x1, y1, token, bno, lno, wno = w
            key = (bno, lno)
            if last_block is None:
                # é¦–è¯ï¼Œç›´æ¥å†™
                parts.append(token)
                start = current_pos
                end = start + len(token)
                entries.append((start, end, (float(x0), float(y0), float(x1), float(y1))))
                current_pos = end
            else:
                if key != (last_block, last_line):
                    # æ¢è¡Œ
                    parts.append("\n")
                    current_pos += 1
                    parts.append(token)
                    start = current_pos
                    end = start + len(token)
                    entries.append((start, end, (float(x0), float(y0), float(x1), float(y1))))
                    current_pos = end
                else:
                    # åŒè¡Œè¯ï¼Œç©ºæ ¼åˆ†éš”
                    parts.append(" ")
                    current_pos += 1
                    parts.append(token)
                    start = current_pos
                    end = start + len(token)
                    entries.append((start, end, (float(x0), float(y0), float(x1), float(y1))))
                    current_pos = end
            last_block, last_line = key
        page_text = "".join(parts)
        return page_text, entries
    except Exception:
        txt = page.get_text() or ""
        return txt, []

def compute_bbox_union_for_range(entries: list, start: int, end: int) -> list:
    """æ ¹æ®è¯çº§ç´¢å¼•åˆ—è¡¨ï¼Œè®¡ç®—è¦†ç›– [start,end) çš„bboxå¹¶é›†ï¼Œè¿”å› [x0,y0,x1,y1]ï¼Œæ— åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚"""
    x0 = y0 = float("inf")
    x1 = y1 = float("-inf")
    found = False
    for (s, e, (a, b, c, d)) in entries:
        if e <= start or s >= end:
            continue
        found = True
        x0 = min(x0, a)
        y0 = min(y0, b)
        x1 = max(x1, c)
        y1 = max(y1, d)
    return [x0, y0, x1, y1] if found else []

# åˆå§‹åŒ–ESå®¢æˆ·ç«¯
es_client = Elasticsearch(
    [f"http://{ES_CONFIG['host']}:{ES_CONFIG['port']}"],
    basic_auth=(ES_CONFIG['username'], ES_CONFIG['password']) if ES_CONFIG['username'] else None,
    verify_certs=ES_CONFIG['verify_certs']
)

# æå®¢API embeddingå‡½æ•°
def get_embedding(text: str) -> list:
    """ä½¿ç”¨æå®¢APIè·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
    try:
        url = GEEKAI_EMBEDDING_URL
        headers = {
            "Authorization": f"Bearer {GEEKAI_API_KEY}",
            "Content-Type": "application/json"
        }
        data = {
            "model": DEFAULT_EMBEDDING_MODEL,
            "input": [text],
            "intent": "search_document"
        }
        response = requests.post(url, headers=headers, json=data, timeout=20)
        if response.status_code == 200:
            return response.json().get("data", [{}])[0].get("embedding")
        else:
            logger.error(f"æå®¢API embeddingå¤±è´¥: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        logger.error(f"æå®¢API embeddingå¼‚å¸¸: {e}")
        return None

# è¯·æ±‚æ¨¡å‹
class LdapValidateRequest(BaseModel):
    username: str
    password: str

class ChatRequest(BaseModel):
    question: str
    user_id: str

class DocumentProcessRequest(BaseModel):
    knowledge_id: int
    knowledge_name: str
    description: str
    tags: List[str]
    effective_time: str

# å“åº”æ¨¡å‹
class LdapValidateResponse(BaseModel):
    success: bool
    message: str
    email: Optional[str] = None
    role: Optional[str] = None

class KnowledgeReference(BaseModel):
    knowledge_id: int
    knowledge_name: str
    description: str
    tags: List[str]
    effective_time: str
    attachments: List[str]
    relevance: float
    # è¿½æº¯å®šä½å¢å¼º
    source_file: Optional[str] = None
    page_num: Optional[int] = None
    chunk_index: Optional[int] = None
    chunk_type: Optional[str] = None
    # æ–°å¢ï¼šè¿”å›å—åæ ‡ä¸å­—ç¬¦èŒƒå›´ï¼Œä¾¿äºå‰ç«¯é«˜äº®
    bbox_union: Optional[List[float]] = None
    char_start: Optional[int] = None
    char_end: Optional[int] = None

class ChatResponse(BaseModel):
    answer: str
    references: List[KnowledgeReference]
    session_id: Optional[str] = None

class DocumentProcessResponse(BaseModel):
    success: bool
    message: str
    chunks_count: int
    knowledge_id: int

@app.get("/")
def read_root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ",
        "version": "2.0.0",
        "features": [
            "PyMuPDF Pro æ–‡æ¡£å¤„ç†",
            "PyMuPDF4LLM ç»“æ„åŒ–åˆ†å—", 
            "LangChain å‘é‡åŒ–",
            "Elasticsearch å­˜å‚¨",
            "æå®¢æ™ºåŠAPI æ™ºèƒ½é—®ç­”"
        ]
    }

# ===== LibreOffice è½¬PDFï¼ˆä¸æ”¹åŠ¨åŸæ–‡ä»¶ï¼‰=====
def _find_soffice_path() -> Optional[str]:
    candidates = [
        r"C:\\Program Files\\LibreOffice\\program\\soffice.exe",
        r"C:\\Program Files (x86)\\LibreOffice\\program\\soffice.exe",
    ]
    for c in candidates:
        if os.path.exists(c):
            return c
    w = which("soffice") or which("soffice.exe")
    return w

def convert_with_libreoffice_safe(src_path: str, timeout_sec: int = 180, out_dir: Optional[str] = None) -> str:
    """ä½¿ç”¨LibreOfficeå°†ä»»æ„Officeæ–‡æ¡£è½¬ä¸ºPDFï¼Œè¾“å‡ºåˆ°ä¸´æ—¶æ–‡ä»¶å¤¹ï¼Œè¿”å›PDFè·¯å¾„ã€‚"""
    soffice = _find_soffice_path()
    if not soffice:
        raise RuntimeError("æœªæ‰¾åˆ°LibreOfficeçš„soffice.exeï¼Œè¯·å®‰è£…æˆ–é…ç½®PATHåé‡è¯•")

    if not out_dir:
        out_dir = tempfile.mkdtemp(prefix="lo_pdf_")
    else:
        os.makedirs(out_dir, exist_ok=True)
    cmd = [
        soffice,
        "--headless",
        "--norestore",
        "--nolockcheck",
        "--convert-to", "pdf",
        "--outdir", out_dir,
        os.path.abspath(src_path),
    ]
    cp = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout_sec)
    if cp.returncode != 0:
        raise RuntimeError(f"LibreOffice è½¬æ¢å¤±è´¥: rc={cp.returncode}\nstdout={cp.stdout}\nstderr={cp.stderr}")

    # æœŸæœ›åŒåpdf
    expected = os.path.join(out_dir, Path(src_path).with_suffix('.pdf').name)
    if os.path.exists(expected):
        return expected
    # å…œåº•æ‰¾æœ€æ–°pdf
    pdfs = [p for p in os.listdir(out_dir) if p.lower().endswith('.pdf')]
    if not pdfs:
        raise RuntimeError(f"LibreOffice æœªç”ŸæˆPDFã€‚stdout={cp.stdout}\nstderr={cp.stderr}")
    pdfs.sort(key=lambda n: os.path.getmtime(os.path.join(out_dir, n)), reverse=True)
    return os.path.join(out_dir, pdfs[0])

def create_simple_pdf_from_txt(src_path: str, out_path: Optional[str] = None) -> str:
    """TXT é™çº§æ–¹æ¡ˆï¼šå°†æ–‡æœ¬ç®€å•æ’ç‰ˆå†™å…¥å•é¡µ/å¤šé¡µPDFï¼Œä¾¿äºåæ ‡å›æ˜¾ã€‚"""
    with open(src_path, 'r', encoding='utf-8', errors='ignore') as f:
        text = f.read()
    out_pdf = out_path or tempfile.mktemp(suffix='.pdf')
    os.makedirs(os.path.dirname(out_pdf), exist_ok=True)
    doc = fitz.open()
    page = doc.new_page(width=595, height=842)
    rect = fitz.Rect(40, 50, 555, 800)
    page.insert_textbox(rect, text[:50000], fontsize=11, fontname="helv", align=0)
    doc.save(out_pdf)
    doc.close()
    return out_pdf

# ===== æŒä¹…åŒ–å­˜å‚¨çš„è½¬æ¢PDFè·¯å¾„ =====
CONVERTED_PDF_ROOT = os.path.join(os.path.dirname(__file__), 'static', 'converted')

def build_converted_pdf_path(knowledge_id: int, original_filename: str) -> str:
    base = Path(original_filename).stem + '.pdf'
    target_dir = os.path.join(CONVERTED_PDF_ROOT, str(knowledge_id))
    os.makedirs(target_dir, exist_ok=True)
    return os.path.join(target_dir, base)

def preprocess_xlsx_for_better_pdf(src_path: str) -> Optional[str]:
    """
    é’ˆå¯¹Excelè¿›è¡Œåˆ†é¡µä¼˜åŒ–ï¼šæ¨ªå‘ã€æŒ‰å®½åº¦é€‚é…ã€è®¾ç½®æ‰“å°åŒºåŸŸä¸è¾¹è·ï¼›
    ç”Ÿæˆä¸´æ—¶xlsxå‰¯æœ¬ï¼ˆä¸æ”¹åŠ¨åŸæ–‡ä»¶ï¼‰ã€‚è¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼›å¤±è´¥è¿”å›Noneã€‚
    """
    if not OPENPYXL_AVAILABLE:
        return None

# ===== Excel åŸç”Ÿè§£æï¼Œä¸è½¬PDF =====
def parse_excel_native(file_path: str, filename: str, knowledge_id: int) -> List[Document]:
    if not OPENPYXL_AVAILABLE:
        raise RuntimeError("ç¼ºå°‘openpyxlï¼Œæ— æ³•åŸç”Ÿè§£æExcelã€‚è¯·å®‰è£…: pip install openpyxl")
    wb = load_workbook(file_path, data_only=True)
    chunks: List[Document] = []

    # æ„å»ºæ¯ä¸ªsheetçš„è™šæ‹Ÿåæ ‡ç½‘æ ¼
    for sheet_idx, ws in enumerate(wb.worksheets):
        # åˆ—å®½ã€è¡Œé«˜ â†’ åƒç´ è¿‘ä¼¼ï¼ˆExcelå®½åº¦å•ä½è½¬åƒç´ ï¼Œè¿™é‡Œåšç®€å•è¿‘ä¼¼ï¼‰
        col_widths = []
        for col in ws.columns:
            # openpyxlåˆ—å®½åœ¨ ws.column_dimensions[col_letter].width
            break
        # æ”¶é›†åˆ—å®½ï¼ˆè‹¥æœªè®¾ç½®ï¼Œç»™é»˜è®¤ 8.43 å­—ç¬¦å®½ â‰ˆ 64pxï¼‰
        from openpyxl.utils import get_column_letter
        max_col = ws.max_column
        max_row = ws.max_row
        for c in range(1, max_col + 1):
            letter = get_column_letter(c)
            cw = ws.column_dimensions.get(letter).width if ws.column_dimensions.get(letter) and ws.column_dimensions.get(letter).width else 8.43
            # è¿‘ä¼¼ï¼š1å­—ç¬¦å®½â‰ˆ7.5px
            col_widths.append(float(cw) * 7.5)
        row_heights = []
        for r in range(1, max_row + 1):
            rh = ws.row_dimensions.get(r).height if ws.row_dimensions.get(r) and ws.row_dimensions.get(r).height else 15.0
            # è¿‘ä¼¼ï¼š1ptâ‰ˆ1.33pxï¼ŒExcelé»˜è®¤è¡Œé«˜â‰ˆ15pt
            row_heights.append(float(rh) * 1.33)

        # å‰ç¼€å’Œå¾—åˆ°å„åˆ—xã€å„è¡Œyèµ·ç‚¹
        x_starts = [0.0]
        for w in col_widths:
            x_starts.append(x_starts[-1] + w)
        y_starts = [0.0]
        for h in row_heights:
            y_starts.append(y_starts[-1] + h)

        # æ„å»ºå•å…ƒæ ¼positions
        positions: List[Dict] = []
        for r in range(1, max_row + 1):
            for c in range(1, max_col + 1):
                cell = ws.cell(row=r, column=c)
                text = str(cell.value) if cell.value is not None else ""
                if not text.strip():
                    continue
                # åˆå¹¶å•å…ƒæ ¼å¤„ç†ï¼šå–æ‰€å±åˆå¹¶åŒºçš„å¤–æ¥çŸ©å½¢
                merged_bbox = None
                for rng in ws.merged_cells.ranges:
                    if (r, c) in rng.cells:
                        min_row, min_col, max_row_, max_col_ = rng.min_row, rng.min_col, rng.max_row, rng.max_col
                        x0 = x_starts[min_col - 1]
                        y0 = y_starts[min_row - 1]
                        x1 = x_starts[max_col_]
                        y1 = y_starts[max_row_]
                        merged_bbox = [x0, y0, x1, y1]
                        break
                if merged_bbox:
                    bbox = merged_bbox
                else:
                    x0 = x_starts[c - 1]
                    y0 = y_starts[r - 1]
                    x1 = x_starts[c]
                    y1 = y_starts[r]
                    bbox = [x0, y0, x1, y1]

                positions.append({
                    "text": text.strip(),
                    "bbox": bbox,
                    "sheet": ws.title,
                    "row": r,
                    "col": c,
                })

        # ç”Ÿæˆçº¯æ–‡æœ¬å†…å®¹ï¼ˆæŒ‰è¡Œæ‹¼æ¥ï¼‰
        lines = []
        for r in range(1, max_row + 1):
            vals = []
            for c in range(1, max_col + 1):
                val = ws.cell(row=r, column=c).value
                vals.append("" if val is None else str(val))
            lines.append("\t".join(vals).rstrip())
        all_text = "\n".join(lines)

        # åˆ†å—ï¼ˆæŒ‰å­—ç¬¦ï¼‰
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        text_chunks = splitter.split_text(all_text)

        # å°†positionsæŒ‰æ–‡æœ¬åŒ…å«å…³ç³»åˆ†é…ç»™chunk
        for chunk_idx, chunk_text in enumerate(text_chunks):
            chunk_positions = []
            # ç®€å•åŒ…å«åŒ¹é…ï¼šå•å…ƒæ ¼æ–‡æœ¬åœ¨chunkä¸­å‡ºç°åˆ™çº³å…¥ï¼ˆå¤§å¤šæ•°è¡¨æ ¼æœ‰æ•ˆï¼‰
            for p in positions:
                t = p["text"]
                if t and t in chunk_text:
                    chunk_positions.append(p)

            # è®¡ç®—å—çº§bboxï¼šæ‰€æœ‰å•å…ƒæ ¼bboxå¹¶é›†ï¼ˆåŒä¸€sheetï¼‰
            if chunk_positions:
                x0 = min(pp["bbox"][0] for pp in chunk_positions)
                y0 = min(pp["bbox"][1] for pp in chunk_positions)
                x1 = max(pp["bbox"][2] for pp in chunk_positions)
                y1 = max(pp["bbox"][3] for pp in chunk_positions)
                bbox = [x0, y0, x1, y1]
            else:
                bbox = [0, 0, 0, 0]

            chunks.append(Document(
                page_content=chunk_text,
                metadata={
                    "knowledge_id": knowledge_id,
                    "source_file": filename,
                    "page_num": sheet_idx + 1,  # å°† sheet å½“ä½œâ€œé¡µâ€
                    "chunk_index": len(chunks),
                    "positions": chunk_positions,
                    "bbox": bbox,
                    "document_name": filename,
                    "document_type": "è¡¨æ ¼",
                    "sheet_name": ws.title,
                    "keywords": extract_keywords_from_content(chunk_text),
                }
            ))

    return chunks

# ===== TXT åŸç”Ÿè§£æï¼Œä¸è½¬PDF =====
def parse_txt_native(file_path: str, filename: str, knowledge_id: int) -> List[Document]:
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read()
    # å½’ä¸€åŒ–æ¢è¡Œ
    content = content.replace('\r\n', '\n').replace('\r', '\n')
    lines = content.split('\n')

    # è¡Œçº§positionsï¼ˆä¸åšçœŸå®åƒç´ åæ ‡ï¼Œè®°å½•è¡Œåˆ—ä¸å­—ç¬¦åŒºé—´ï¼‰
    positions: List[Dict] = []
    global_pos = 0
    for i, line in enumerate(lines, start=1):
        text = line
        start = global_pos
        end = start + len(text)
        positions.append({
            "text": text,
            "line_no": i,
            "char_start": start,
            "char_end": end,
        })
        global_pos = end + 1  # è®¡å…¥æ¢è¡Œ

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    text_chunks = splitter.split_text(content)

    chunks: List[Document] = []
    for idx, chunk_text in enumerate(text_chunks):
        # ä¸ºchunkåˆ†é…è¡Œæ®µpositions
        chunk_positions = []
        # é€šè¿‡å­—ç¬¦èŒƒå›´ç²—åŒ¹é…ï¼ˆè¡Œæ–‡æœ¬åœ¨chunkä¸­å‡ºç°æ—¶çº³å…¥ï¼‰
        for p in positions:
            t = p["text"]
            if t and t in chunk_text:
                chunk_positions.append(p)
        chunks.append(Document(
            page_content=chunk_text,
            metadata={
                "knowledge_id": knowledge_id,
                "source_file": filename,
                "page_num": 1,
                "chunk_index": idx,
                "positions": chunk_positions,
                "bbox": [],  # æ–‡æœ¬è§†å›¾é€šå¸¸ä¸éœ€è¦åƒç´ åæ ‡
                "document_name": filename,
                "document_type": "æ–‡æœ¬",
                "keywords": extract_keywords_from_content(chunk_text),
            }
        ))

    return chunks
    try:
        wb = load_workbook(src_path, data_only=True)
        for ws in wb.worksheets:
            # æ¨ªå‘æ‰“å° + å®½åº¦é€‚é…
            ws.page_setup.orientation = 'landscape'
            ws.page_setup.fitToWidth = 1
            ws.page_setup.fitToHeight = 0
            if ws.sheet_properties.pageSetUpPr is None:
                ws.sheet_properties.pageSetUpPr = PageSetupProperties(fitToPage=True)
            else:
                ws.sheet_properties.pageSetUpPr.fitToPage = True
            # å±…ä¸­ä¸è¾¹è·
            ws.print_options.horizontalCentered = True
            ws.page_margins.left = 0.5
            ws.page_margins.right = 0.5
            ws.page_margins.top = 0.6
            ws.page_margins.bottom = 0.6
            # æ‰“å°åŒºåŸŸè¦†ç›–å·²ç”¨èŒƒå›´
            try:
                dim = ws.calculate_dimension()  # å¦‚ 'A1:G200'
                ws.print_area = dim
            except Exception:
                pass
        fd, tmp_path = tempfile.mkstemp(suffix='.xlsx')
        os.close(fd)
        wb.save(tmp_path)
        return tmp_path
    except Exception as e:
        logger.warning(f"Excelé¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡ä»¶è½¬æ¢: {e}")
        return None

@app.post("/api/ldap/validate", response_model=LdapValidateResponse)
def validate_ldap_user(request: LdapValidateRequest):
    """LDAPç”¨æˆ·éªŒè¯ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    logger.info(f"LDAPéªŒè¯è¯·æ±‚: {request.username}")
    
    # æ¨¡æ‹ŸLDAPéªŒè¯
    if request.username == "admin" and request.password == "password":
        return LdapValidateResponse(
            success=True,
            message="éªŒè¯æˆåŠŸ",
            email="admin@example.com",
            role="admin"
        )
    else:
        return LdapValidateResponse(
            success=False,
            message="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"
        )

def process_document_unified(
    file_path: str,
    filename: str,
    knowledge_id: int,
    knowledge_name: Optional[str] = None,
    description: Optional[str] = None,
    tags: Optional[str] = None,
    effective_time: Optional[str] = None,
) -> Dict:
    """
    ç»Ÿä¸€å¤„ç†æ–‡æ¡£ï¼š
    - éPDFå…ˆè½¬æ¢ä¸ºPDFï¼ˆä¼˜å…ˆä½¿ç”¨LibreOfficeï¼‰ï¼Œä¸æ”¹åŠ¨åŸå§‹æ–‡ä»¶
    - åŸºäºPDFç”¨PyMuPDFæå–å—çº§åæ ‡å¹¶åˆ†å—
    """
    try:
        # 1) éPDF â†’ PDFï¼ˆä¸æ”¹åŠ¨åŸå§‹æ–‡ä»¶ï¼Œä»…ç”Ÿæˆä¸´æ—¶PDFç”¨äºå®šä½ä¸å›æ˜¾ï¼‰
        ext = Path(filename).suffix.lower()
        pdf_path_to_open = file_path
        temp_generated_pdf = None

        if ext in {'.xlsx', '.xls'}:
            # èµ°ExcelåŸç”Ÿè§£æï¼Œä¸è½¬PDF
            chunks = parse_excel_native(file_path, filename, knowledge_id)
        elif ext == '.txt':
            # èµ°TXTåŸç”Ÿè§£æï¼Œä¸è½¬PDF
            chunks = parse_txt_native(file_path, filename, knowledge_id)
        elif ext != '.pdf':
            # ç”ŸæˆæŒä¹…åŒ–çš„ç›®æ ‡PDFè·¯å¾„ï¼ˆä¾›å‰ç«¯ä¸‹è½½/å›æ˜¾ç”¨ï¼‰
            target_pdf = build_converted_pdf_path(int(knowledge_id) if knowledge_id else 0, filename)
            try:
                # ç›´æ¥è¾“å‡ºåˆ°ç›®æ ‡ç›®å½•ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶è¢«åˆ 
                target_dir = os.path.dirname(target_pdf)
                src_for_convert = file_path
                # é’ˆå¯¹xlsx/xlsè¿›è¡Œåˆ†é¡µä¼˜åŒ–é¢„å¤„ç†
                if ext in {'.xlsx', '.xls'}:
                    preprocessed = preprocess_xlsx_for_better_pdf(file_path) if ext == '.xlsx' else None
                    if preprocessed and os.path.exists(preprocessed):
                        src_for_convert = preprocessed
                        logger.info(f"Excelé¢„å¤„ç†å®Œæˆï¼Œä½¿ç”¨å‰¯æœ¬è½¬æ¢: {src_for_convert}")

                pdf_path_to_open = convert_with_libreoffice_safe(src_for_convert, out_dir=target_dir)
                # è‹¥è¾“å‡ºåä¸æœŸæœ›ä¸ä¸€è‡´ï¼Œå¤åˆ¶ä¸€ä»½ä¸ºæœŸæœ›å
                if os.path.abspath(pdf_path_to_open) != os.path.abspath(target_pdf):
                    shutil.copyfile(pdf_path_to_open, target_pdf)
                    pdf_path_to_open = target_pdf
                logger.info(f"å·²å°† {filename} è½¬ä¸ºPDF: {pdf_path_to_open}")
            except Exception as e:
                # å¯¹çº¯æ–‡æœ¬åšé™çº§ï¼šç›´æ¥ç”Ÿæˆç®€å•PDFåˆ°ç›®æ ‡è·¯å¾„
                if ext == '.txt':
                    pdf_path_to_open = create_simple_pdf_from_txt(file_path, out_path=target_pdf)
                    logger.info(f"TXTé™çº§è½¬PDFæˆåŠŸ: {pdf_path_to_open}")
                else:
                    raise

        # 2) åŸºäºPDFèµ°ç»Ÿä¸€è§£æ
        doc = fitz.open(pdf_path_to_open)
        try:
            logger.info(f"æˆåŠŸæ‰“å¼€æ–‡æ¡£ï¼Œé¡µæ•°: {len(doc)}")

            documents = extract_documents_with_block_positions(doc, filename)

            all_content = ""
            all_positions = []
            for doc_info in documents:
                all_content += doc_info["content"] + "\n"
                all_positions.extend(doc_info["positions"])

            logger.info(f"åˆå¹¶åæ€»å†…å®¹é•¿åº¦: {len(all_content)} å­—ç¬¦")
            logger.info(f"åˆå¹¶åæ€»ä½ç½®ä¿¡æ¯æ•°é‡: {len(all_positions)}")

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
            )

            text_chunks = text_splitter.split_text(all_content)
            logger.info(f"LangChainåˆ†å‰²åç”Ÿæˆ {len(text_chunks)} ä¸ªchunks")

            chunks = []
            for chunk_idx, chunk_text in enumerate(text_chunks):
                chunk_positions = assign_positions_to_chunk(chunk_text, all_positions)

                page_counts: Dict[int, int] = {}
                for p in chunk_positions:
                    pg = int(p.get('page', 1))
                    page_counts[pg] = page_counts.get(pg, 0) + 1
                main_page = max(page_counts.items(), key=lambda kv: kv[1])[0] if page_counts else 1

                chunk = Document(
                    page_content=chunk_text,
                    metadata={
                        "knowledge_id": knowledge_id,
                        "source_file": filename,
                        "page_num": main_page,
                        "chunk_index": chunk_idx,
                        "positions": chunk_positions,
                        "bbox": calculate_chunk_bbox(chunk_positions),
                        "document_name": filename,
                        "document_type": "æ–‡æ¡£",
                        "keywords": extract_keywords_from_content(chunk_text),
                        # ååŒåˆ°ES
                        "knowledge_name": knowledge_name or "",
                        "description": description or "",
                        "tags": tags or "",
                        "effective_time": effective_time or "",
                    }
                )
                chunks.append(chunk)
        finally:
            try:
                doc.close()
            except Exception:
                pass
            # ä¸åˆ é™¤æŒä¹…åŒ–PDFï¼Œä¾›å‰ç«¯ä¸‹è½½/å›æ˜¾
        
        # ä¸ºExcel/TXTç­‰åŸç”Ÿè§£æç”Ÿæˆçš„chunkè¡¥é½çŸ¥è¯†å…ƒæ•°æ®å­—æ®µ
        try:
            for ch in chunks:
                md = ch.metadata
                if "knowledge_name" not in md:
                    md["knowledge_name"] = knowledge_name or ""
                if "description" not in md:
                    md["description"] = description or ""
                if "tags" not in md:
                    md["tags"] = tags or ""
                if "effective_time" not in md:
                    md["effective_time"] = effective_time or ""
        except Exception:
            pass

        logger.info(f"æœ€ç»ˆç”Ÿæˆ {len(chunks)} ä¸ªchunks")
        
        # å­˜å‚¨åˆ°ES
        stored_count = store_chunks_to_es(chunks, knowledge_id)
        
        return {
            "chunks_count": stored_count,
            "total_chunks": len(chunks),
            "success": stored_count > 0
        }
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise e

def extract_documents_with_block_positions(doc, filename: str) -> List[Dict]:
    """
    ç›´æ¥ä»PyMuPDFæå–æ–‡æ¡£å—å’Œä½ç½®ä¿¡æ¯
    """
    documents = []

    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        blocks = page.get_text("dict")

        for block_idx, block in enumerate(blocks["blocks"]):
            if "lines" in block:
                # æ”¶é›†æ•´ä¸ªå—çš„æ‰€æœ‰æ–‡æœ¬å’Œä½ç½®ä¿¡æ¯
                block_text = ""
                block_positions = []

                for line_idx, line in enumerate(block["lines"]):
                    for span_idx, span in enumerate(line["spans"]):
                        text = span["text"].strip()
                        if text:
                            block_text += text + " "
                            block_positions.append({
                                "text": text,
                                "bbox": span["bbox"],
                                "font_size": span["size"],
                                "font": span["font"],
                                "span_idx": span_idx,
                                "line_idx": line_idx,
                                "page": page_num + 1,
                            })

                if block_text.strip():
                    # è®¡ç®—æ•´ä¸ªå—çš„è¾¹ç•Œæ¡†ï¼ˆå†…è”å®ç°ï¼‰
                    if block_positions:
                        bx0 = min(p["bbox"][0] for p in block_positions)
                        by0 = min(p["bbox"][1] for p in block_positions)
                        bx1 = max(p["bbox"][2] for p in block_positions)
                        by1 = max(p["bbox"][3] for p in block_positions)
                        block_bbox = [bx0, by0, bx1, by1]
                    else:
                        block_bbox = [0, 0, 0, 0]

                    documents.append({
                        "content": block_text.strip(),
                        "page": page_num + 1,
                        "block_idx": block_idx,
                        "positions": block_positions,
                        "bbox": block_bbox
                    })

    return documents


def assign_positions_to_chunk(chunk_text: str, positions: List[Dict]) -> List[Dict]:
    """
    ä¸ºchunkåˆ†é…å¯¹åº”çš„ä½ç½®ä¿¡æ¯
    ä½¿ç”¨ç®€å•çš„æ–‡æœ¬åŒ…å«å…³ç³»ï¼Œè€Œä¸æ˜¯å¤æ‚çš„ç›¸ä¼¼åº¦è®¡ç®—
    """
    chunk_positions = []

    for pos in positions:
        pos_text = pos["text"]
        # å¦‚æœchunkåŒ…å«è¿™ä¸ªä½ç½®çš„æ–‡æœ¬ï¼Œå°±åˆ†é…ç»™å®ƒ
        if pos_text in chunk_text:
            chunk_positions.append(pos)

    return chunk_positions

def calculate_chunk_bbox(positions: List[Dict]) -> List[float]:
    """è®¡ç®—chunkçš„è¾¹ç•Œæ¡†ï¼šå…ˆæŒ‰spanæ‰€åœ¨é¡µèšç±»ï¼Œå–ä½ç½®æœ€å¤šçš„é¡µä½œä¸ºä¸»é¡µé¢ï¼Œå†å¯¹è¯¥é¡µçš„positionsæ±‚å¹¶é›†"""
    if not positions:
        return [0, 0, 0, 0]

    # ä½ç½®é‡Œéœ€è¦å¸¦ä¸Špageä¿¡æ¯ï¼›è‹¥æ²¡æœ‰ï¼Œé»˜è®¤é¡µ=1
    page_to_positions: Dict[int, List[Dict]] = {}
    for pos in positions:
        page = int(pos.get('page', 1)) if isinstance(pos, dict) else 1
        page_to_positions.setdefault(page, []).append(pos)

    # é€‰æ‹©ä½ç½®æœ€å¤šçš„é¡µä½œä¸ºä¸»é¡µé¢
    main_page = max(page_to_positions.items(), key=lambda kv: len(kv[1]))[0]
    main_positions = page_to_positions[main_page]

    x0 = min(p["bbox"][0] for p in main_positions)
    y0 = min(p["bbox"][1] for p in main_positions)
    x1 = max(p["bbox"][2] for p in main_positions)
    y1 = max(p["bbox"][3] for p in main_positions)

    return [x0, y0, x1, y1]

def generate_pdfllm_style_markdown(doc, filename: str) -> tuple[str, List[Dict]]:
    """
    ä½¿ç”¨PyMuPDFç”Ÿæˆå¹²å‡€çš„Markdownå†…å®¹å’Œç²¾ç¡®çš„æ–‡æœ¬-åæ ‡æ˜ å°„
    è¿”å›: (markdown_content, position_mapping)
    """
    content_lines = []
    position_mapping = []
    
    # æ·»åŠ æ–‡æ¡£æ ‡é¢˜
    content_lines.append(f"# {filename}")
    content_lines.append("")
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        
        # é¡µé¢æ ‡é¢˜
        content_lines.append(f"## ç¬¬ {page_num + 1} é¡µ")
        content_lines.append("")
        
        # è·å–é¡µé¢æ–‡æœ¬å—
        blocks = page.get_text("dict")
        
        if "blocks" in blocks:
            for block_idx, block in enumerate(blocks["blocks"]):
                if "lines" in block:
                    block_text = ""
                    block_positions = []
                    
                    for line_idx, line in enumerate(block["lines"]):
                        line_text = ""
                        line_positions = []
                        
                        for span_idx, span in enumerate(line["spans"]):
                            text = span["text"].strip()
                            if text:
                                # åªæ·»åŠ æ–‡æœ¬å†…å®¹ï¼Œä¸æ·»åŠ ä½ç½®æ ‡ç­¾
                                line_text += text + " "
                                
                                # è®°å½•ä½ç½®ä¿¡æ¯
                                line_positions.append({
                                    "text": text,
                                    "page": page_num + 1,
                                    "bbox": span["bbox"],
                                    "font_size": span["size"],
                                    "font": span["font"],
                                    "block_idx": block_idx,
                                    "line_idx": line_idx,
                                    "span_idx": span_idx,
                                    "char_start": len("".join(content_lines)),  # åœ¨æœ€ç»ˆæ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®
                                    "char_end": len("".join(content_lines)) + len(text)  # åœ¨æœ€ç»ˆæ–‡æœ¬ä¸­çš„ç»“æŸä½ç½®
                                })
                        
                        if line_text.strip():
                            block_text += line_text.strip() + "\n"
                            block_positions.extend(line_positions)
                    
                    if block_text.strip():
                        # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯æ ‡é¢˜ï¼ˆåŸºäºå­—ä½“å¤§å°ï¼‰
                        max_font_size = max(span["size"] for line in block["lines"] for span in line["spans"])
                        if max_font_size > 12:  # å‡è®¾å¤§äº12ptçš„æ˜¯æ ‡é¢˜
                            content_lines.append(f"### {block_text.strip()}")
                        else:
                            content_lines.append(block_text.strip())
                        content_lines.append("")
                        
                        # æ·»åŠ ä½ç½®ä¿¡æ¯åˆ°æ˜ å°„
                        position_mapping.extend(block_positions)
    
    # åˆå¹¶æ‰€æœ‰å†…å®¹
    markdown_content = "\n".join(content_lines)
    
    # æ›´æ–°å­—ç¬¦ä½ç½®ï¼ˆå› ä¸ºæ¢è¡Œç¬¦ç­‰ä¼šå½±å“ä½ç½®ï¼‰
    current_pos = 0
    for pos_info in position_mapping:
        text = pos_info["text"]
        pos_info["char_start"] = current_pos
        pos_info["char_end"] = current_pos + len(text)
        current_pos += len(text) + 1  # +1 for space
    
    return markdown_content, position_mapping

def extract_position_mapping(doc) -> List[Dict]:
    """
    å•ç‹¬æå–ä½ç½®ä¿¡æ¯æ˜ å°„ï¼Œä¸åµŒå…¥åˆ°æ–‡æœ¬å†…å®¹ä¸­
    """
    position_mapping = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        blocks = page.get_text("dict")
        
        if "blocks" in blocks:
            for block_idx, block in enumerate(blocks["blocks"]):
                if "lines" in block:
                    for line_idx, line in enumerate(block["lines"]):
                        for span_idx, span in enumerate(line["spans"]):
                            text = span["text"].strip()
                            if text:
                                position_mapping.append({
                                    "text": text,
                                    "page": page_num + 1,
                                    "bbox": span["bbox"],
                                    "font_size": span["size"],
                                    "font": span["font"],
                                    "block_idx": block_idx,
                                    "line_idx": line_idx,
                                    "span_idx": span_idx
                                })
    
    return position_mapping

def find_best_position_match(chunk_text: str, position_mapping: List[Dict]) -> Optional[Dict]:
    """
    ä¸ºchunkæ‰¾åˆ°æœ€åŒ¹é…çš„ä½ç½®ä¿¡æ¯
    ä½¿ç”¨å—çº§æ¨¡ç³ŠåŒ¹é…ï¼Œè€Œä¸æ˜¯ç²¾ç¡®çš„æ–‡æœ¬ç‰‡æ®µåŒ¹é…
    """
    if not position_mapping:
        return None
    
    chunk_text_lower = chunk_text.lower()
    chunk_words = set(chunk_text_lower.split())
    
    # å®šä¹‰å…³é”®ä¿¡æ¯å…³é”®è¯å’Œå¯¹åº”çš„ä¼˜å…ˆçº§
    key_phrases_priority = [
        # é«˜ä¼˜å…ˆçº§ï¼šåŸºé‡‘æ ¸å¿ƒä¿¡æ¯
        (["åŸºé‡‘æ€»å€¼", "4.4377", "äº¿ç¾å…ƒ"], 10),
        (["åŸºé‡‘ä»·æ ¼", "èµ„äº§å‡€å€¼", "5.7741"], 9),
        (["æˆç«‹æ—¥æœŸ", "2010å¹´8æœˆ2æ—¥"], 8),
        (["åŸºé‡‘ç»ç†", "Justin Kass", "David Oberto", "Michael Yee"], 7),
        (["ç®¡ç†è´¹", "1.19%"], 6),
        (["æŠ•èµ„ç›®æ ‡", "ç¾å›½å€ºåˆ¸", "é«˜æ”¶ç›Š"], 5),
        (["æ”¶ç›Šåˆ†é…", "æ¯æœˆ"], 4),
        (["è´¢æ”¿å¹´åº¦", "9æœˆ30æ—¥"], 3),
        (["äº¤æ˜“æ—¥", "æ¯æ—¥"], 2),
        (["æŠ•èµ„ç»ç†", "å®‰è”æŠ•èµ„"], 1)
    ]
    
    # ç¬¬ä¸€è½®ï¼šä¼˜å…ˆåŒ¹é…åŒ…å«å…³é”®ä¿¡æ¯çš„æ–‡æœ¬
    for pos_info in position_mapping:
        text = pos_info.get("text", "").strip()
        if text:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜ä¼˜å…ˆçº§çš„å…³é”®ä¿¡æ¯
            for key_phrases, priority in key_phrases_priority:
                if any(keyword in text for keyword in key_phrases):
                    # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
                    text_lower = text.lower()
                    chunk_lower = chunk_text_lower
                    
                    # ä½¿ç”¨å­—ç¬¦é‡å åº¦è®¡ç®—ç›¸ä¼¼åº¦
                    overlap = sum(1 for c in text_lower if c in chunk_lower)
                    base_score = overlap / max(len(text_lower), 1)
                    
                    # åº”ç”¨ä¼˜å…ˆçº§æƒé‡
                    weighted_score = base_score * priority
                    
                    if weighted_score > best_score:
                        best_score = weighted_score
                        best_match = pos_info
                    break  # æ‰¾åˆ°åŒ¹é…çš„å…³é”®è¯å°±è·³å‡ºå†…å±‚å¾ªç¯
    
    # ç¬¬äºŒè½®ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®ä¿¡æ¯ï¼Œå°è¯•ç²¾ç¡®åŒ…å«åŒ¹é…
    if not best_match:
        for pos_info in position_mapping:
            text = pos_info.get("text", "").strip()
            if text and text.lower() in chunk_text_lower:
                # è®¡ç®—åŒ¹é…åº¦ï¼šæ–‡æœ¬é•¿åº¦ä¸chunké•¿åº¦çš„æ¯”ä¾‹
                score = len(text) / max(len(chunk_text), 1)
                if score > best_score:
                    best_score = score
                    best_match = pos_info
    
    # ç¬¬ä¸‰è½®ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å•è¯çº§åˆ«çš„åŒ¹é…
    if not best_match:
        for pos_info in position_mapping:
            text = pos_info.get("text", "").strip()
            if text:
                text_words = set(text.lower().split())
                # è®¡ç®—å•è¯é‡å åº¦
                overlap = len(chunk_words.intersection(text_words))
                if overlap > 0:
                    score = overlap / max(len(chunk_words), 1)
                    if score > best_score:
                        best_score = score
                        best_match = pos_info
    
    # ç¬¬å››è½®ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ä½ç½®ä¿¡æ¯
    if not best_match and position_mapping:
        for pos_info in position_mapping:
            if pos_info.get("bbox") and len(pos_info.get("bbox", [])) == 4:
                return pos_info
    
    return best_match

def expand_bbox_to_block_level(bbox: List[float], page_width: float, page_height: float) -> List[float]:
    """
    å°†ç²¾ç¡®çš„æ–‡æœ¬ç‰‡æ®µbboxæ‰©å±•ä¸ºå—çº§bboxï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
    ä½¿ç”¨æ›´ä¿å®ˆçš„æ‰©å±•ç­–ç•¥ï¼Œé¿å…è¿‡åº¦æ‰©å±•
    """
    if len(bbox) != 4:
        return bbox
    
    x0, y0, x1, y1 = bbox
    
    # è®¡ç®—å½“å‰æ–‡æœ¬çš„å®½åº¦å’Œé«˜åº¦
    text_width = x1 - x0
    text_height = y1 - y0
    
    # æ›´ä¿å®ˆçš„æ‰©å±•ç­–ç•¥
    # æ°´å¹³æ‰©å±•ï¼šå·¦å³å„æ‰©å±•æ–‡æœ¬å®½åº¦çš„50%ï¼Œä½†ä¸è¶…è¿‡é¡µé¢è¾¹è·
    horizontal_expansion = min(text_width * 0.5, 30)  # æœ€å¤§æ‰©å±•30åƒç´ 
    
    expanded_x0 = max(20, x0 - horizontal_expansion)
    expanded_x1 = min(page_width - 20, x1 + horizontal_expansion)
    
    # å‚ç›´æ‰©å±•ï¼šä¸Šä¸‹å„æ‰©å±•æ–‡æœ¬é«˜åº¦çš„50%ï¼Œä½†ä¸è¶…è¿‡é¡µé¢è¾¹è·
    vertical_expansion = min(text_height * 0.5, 20)  # æœ€å¤§æ‰©å±•20åƒç´ 
    
    expanded_y0 = max(20, y0 - vertical_expansion)
    expanded_y1 = min(page_height - 20, y1 + vertical_expansion)
    
    return [expanded_x0, expanded_y0, expanded_x1, expanded_y1]

def find_best_position_match_block_level(chunk_text: str, position_mapping: List[Dict], page_width: float, page_height: float) -> Optional[Dict]:
    """
    ä¸ºchunkæ‰¾åˆ°æœ€åŒ¹é…çš„ä½ç½®ä¿¡æ¯ï¼Œå¹¶æ‰©å±•ä¸ºå—çº§åæ ‡
    """
    # å…ˆæ‰¾åˆ°æœ€ä½³åŒ¹é…
    best_match = find_best_position_match(chunk_text, position_mapping)
    
    if best_match and best_match.get("bbox"):
        # æ‰©å±•bboxä¸ºå—çº§åæ ‡
        expanded_bbox = expand_bbox_to_block_level(best_match["bbox"], page_width, page_height)
        
        # åˆ›å»ºæ–°çš„ä½ç½®ä¿¡æ¯ï¼ŒåŒ…å«æ‰©å±•åçš„åæ ‡
        block_level_match = best_match.copy()
        block_level_match["bbox"] = expanded_bbox
        block_level_match["bbox_type"] = "block_level"  # æ ‡è®°ä¸ºå—çº§åæ ‡
        
        return block_level_match
    
    return best_match

def store_chunks_to_es(chunks: List[Document], knowledge_id: int):
    """
    å°†chunkså­˜å‚¨åˆ°Elasticsearch
    """
    stored_count = 0
    
    for i, chunk in enumerate(chunks):
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = hashlib.md5(f"{knowledge_id}_{i}_{chunk.page_content[:100]}".encode()).hexdigest()
            
            # ä¸ºchunkç”Ÿæˆembedding
            chunk_embedding = get_embedding(chunk.page_content)
            if not chunk_embedding:
                logger.warning(f"Chunk {i} embeddingç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            # å‡†å¤‡ESæ–‡æ¡£
            es_doc = {
                "content": chunk.page_content,
                "embedding": chunk_embedding,
                "knowledge_id": chunk.metadata.get("knowledge_id", knowledge_id),
                "knowledge_name": chunk.metadata.get("knowledge_name", ""),
                "description": chunk.metadata.get("description", ""),
                "tags": chunk.metadata.get("tags", ""),
                "effective_time": chunk.metadata.get("effective_time", ""),
                "source_file": chunk.metadata.get("source_file", ""),
                "chunk_index": chunk.metadata.get("chunk_index", i),
                "chunk_type": chunk.metadata.get("chunk_type", "content"),
                "page_num": chunk.metadata.get("page_num", 1),
                "bbox": chunk.metadata.get("bbox", []),
                "positions": chunk.metadata.get("positions", []),
                "weight": 1.0
            }
            
            # å­˜å‚¨åˆ°ES
            es_client.index(index=ES_CONFIG['index'], id=doc_id, document=es_doc)
            stored_count += 1
            
            if (i + 1) % 10 == 0:
                logger.info(f"å·²å­˜å‚¨ {i + 1}/{len(chunks)} ä¸ªchunks")
                
        except Exception as e:
            logger.error(f"å­˜å‚¨chunk {i} å¤±è´¥: {e}")
            continue

    logger.info(f"ESå­˜å‚¨å®Œæˆï¼ŒæˆåŠŸå­˜å‚¨ {stored_count}/{len(chunks)} ä¸ªchunks")
    return stored_count

@app.post("/api/document/process", response_model=DocumentProcessResponse)
async def process_document(
    file: UploadFile = File(...),
    knowledge_id: int = Form(None),
    knowledge_name: str = Form(None),
    description: str = Form(None),
    tags: str = Form(None),
    effective_time: str = Form(None)
):
    """
    ä½¿ç”¨ PyMuPDF Pro + PyMuPDF4LLM ç»Ÿä¸€å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£
    æ”¯æŒ PDFã€Wordã€Excelã€PowerPointã€TXT ç­‰æ ¼å¼
    """
    logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file.filename}, çŸ¥è¯†ID: {knowledge_id}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        file_extension = Path(file.filename).suffix.lower()
        
        # PyMuPDF Pro æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {
            ".pdf", ".docx", ".doc", ".xlsx", ".xls", 
            ".pptx", ".ppt", ".txt", ".hwp", ".hwpx"
        }
        
        if file_extension not in supported_extensions:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}")
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # å¤„ç†æ–‡æ¡£
            result = process_document_unified(
                temp_file_path,
                file.filename,
                knowledge_id,
                knowledge_name=knowledge_name,
                description=description,
                tags=tags,
                effective_time=effective_time,
            )
            
            return DocumentProcessResponse(
                success=True,
                message=f"æ–‡æ¡£å¤„ç†æˆåŠŸ: {file.filename}",
                chunks_count=result["chunks_count"],
                knowledge_id=int(knowledge_id) if knowledge_id is not None else 0
            )
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file_path)
            
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

@app.post("/api/rag/chat", response_model=ChatResponse)
def chat_with_rag(request: ChatRequest):
    """
    åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”
    """
    logger.info(f"RAGèŠå¤©è¯·æ±‚: {request.question}")
    
    try:
        # 1. å‘é‡æœç´¢æ‰¾åˆ°ç›¸å…³chunks
        question_embedding = get_embedding(request.question)
        if not question_embedding:
            return ChatResponse(
                answer="æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆé—®é¢˜çš„å‘é‡è¡¨ç¤ºï¼Œè¯·é‡è¯•ã€‚",
                references=[],
                session_id=request.user_id
            )
        
        # ç®€åŒ–æœç´¢é€»è¾‘ - ç›´æ¥è¿”å›è¯­ä¹‰ç›¸ä¼¼åº¦æœ€é«˜çš„chunks
        # ä»…æ£€ç´¢åŒ…å«embeddingå­—æ®µçš„æ–‡æ¡£ï¼Œé¿å…è„šæœ¬åœ¨ç¼ºå¤±å­—æ®µæ—¶æŠ¥é”™
        search_query = {
            "size": 5,  # è¿”å›å‰5ä¸ªæœ€ç›¸å…³çš„chunks
            "query": {
                "script_score": {
                    "query": {
                        "bool": {
                            "filter": [
                                {"exists": {"field": "embedding"}}
                            ]
                        }
                    },
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": question_embedding}
                    }
                }
            },
            "_source": [
                "content",
                "knowledge_id",
                "knowledge_name",
                "description",
                "tags",
                "effective_time",
                "source_file",
                "page_num",
                "chunk_index",
                "bbox",
                "positions"
            ]
        }
        
        search_response = es_client.search(index=ES_CONFIG['index'], body=search_query)
        hits = search_response.get('hits', {}).get('hits', [])
        
        if not hits:
            return ChatResponse(
                answer="æŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                references=[],
                session_id=request.user_id
            )
        
        # 2. æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_chunks = []
        for hit in hits:
            source = hit['_source']
            score = hit['_score']
            
            # æ„å»ºæ¯ä¸ªchunkçš„å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯
            chunk_info = {
                "content": source.get('content', ''),
                "metadata": {
                    "knowledge_id": source.get('knowledge_id', 0),
                    "knowledge_name": source.get('knowledge_name', ''),
                    "description": source.get('description', ''),
                    "tags": source.get('tags', ''),
                    "effective_time": source.get('effective_time', ''),
                    "document_name": source.get('source_file', 'N/A'),
                    "document_type": "æ–‡æ¡£",  # é€šç”¨æ–‡æ¡£ç±»å‹
                    "page_num": source.get('page_num', 'N/A'),
                    "chunk_index": source.get('chunk_index', 'N/A'),
                    "bbox": source.get('bbox', []),
                    "positions": source.get('positions', []),
                    "relevance_score": round(score, 3)
                }
            }
            context_chunks.append(chunk_info)
        
        # 3. æ„å»ºå¢å¼ºçš„RAGæç¤ºè¯
        enhanced_prompt = build_enhanced_rag_prompt(request.question, context_chunks)
        
        # 4. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
        answer = generate_ai_answer(enhanced_prompt)
        
        # 5. æ„å»ºå¼•ç”¨ä¿¡æ¯
        references = []
        for chunk in context_chunks:
            metadata = chunk['metadata']
            references.append(KnowledgeReference(
                knowledge_id=int(metadata.get('knowledge_id', 0)) if metadata.get('knowledge_id') is not None else 0,
                knowledge_name=metadata.get('knowledge_name', ''),
                description=metadata.get('description', ''),
                tags=[metadata.get('tags', '')] if isinstance(metadata.get('tags', ''), str) else metadata.get('tags', []),
                effective_time=metadata.get('effective_time', ''),
                attachments=[metadata.get('document_name', '')],
                relevance=metadata.get('relevance_score', 0.0),
                source_file=metadata.get('document_name', ''),
                page_num=metadata.get('page_num', 0),
                chunk_index=metadata.get('chunk_index', 0),
                chunk_type="content",
                bbox_union=metadata.get('bbox', []),  # ä½¿ç”¨æ–°çš„bboxå­—æ®µ
                char_start=0,  # ä¸å†ä½¿ç”¨å­—ç¬¦ä½ç½®
                char_end=0
            ))
        
        return ChatResponse(
            answer=answer,
            references=references,
            session_id=request.user_id
        )
        
    except Exception as e:
        logger.error(f"RAGèŠå¤©å¤±è´¥: {e}")
        return ChatResponse(
            answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
            references=[],
            session_id=request.user_id
        )

def build_enhanced_rag_prompt(question: str, context_chunks: List[Dict]) -> str:
    """
    æ„å»ºå¢å¼ºçš„RAGæç¤ºè¯ï¼Œè®©å¤§æ¨¡å‹èƒ½çœ‹åˆ°å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    context_parts = []
    
    for i, chunk in enumerate(context_chunks):
        metadata = chunk['metadata']
        
        # æ„å»ºæ¯ä¸ªchunkçš„è¯¦ç»†ä¸Šä¸‹æ–‡ä¿¡æ¯
        chunk_context = f"""
=== å¼•ç”¨ {i+1} ===
ğŸ“„ æ–‡æ¡£åç§°: {metadata.get('document_name', 'N/A')}
ğŸ“‹ æ–‡æ¡£ç±»å‹: {metadata.get('document_type', 'N/A')}
ğŸ“– é¡µç : {metadata.get('page_num', 'N/A')}
ğŸ”¢ å—åº: {metadata.get('chunk_index', 'N/A')}
ğŸ¯ ç›¸å…³æ€§: {metadata.get('relevance_score', 'N/A')}
ğŸ“ åæ ‡: {metadata.get('bbox', [])}
ğŸ“ å†…å®¹: {chunk.get('content', '')}
"""
        context_parts.append(chunk_context)
    
    # æ„å»ºå®Œæ•´æç¤º
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

æ¯ä¸ªå¼•ç”¨éƒ½åŒ…å«äº†å®Œæ•´çš„æ–‡æ¡£ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æ¡£åç§°å’Œç±»å‹
- é¡µç å’Œå—åº
- ç›¸å…³æ€§è¯„åˆ†å’Œä½ç½®åæ ‡
- å…·ä½“å†…å®¹

è¯·ä»”ç»†åˆ†æè¿™äº›ä¿¡æ¯ï¼Œå¹¶æ ¹æ®é—®é¢˜æ‰¾åˆ°æœ€å‡†ç¡®çš„ç­”æ¡ˆã€‚

é—®é¢˜: {question}

å‚è€ƒä¿¡æ¯:
{''.join(context_parts)}

è¯·æ ¹æ®é—®é¢˜ï¼Œä»ä¸Šè¿°ä¿¡æ¯ä¸­æ‰¾åˆ°æœ€å‡†ç¡®çš„ç­”æ¡ˆã€‚è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦è¯´"è¯·æŸ¥çœ‹å¼•ç”¨ä¿¡æ¯"
2. å¦‚æœé—®é¢˜æ¶‰åŠç‰¹å®šæ–‡æ¡£ï¼Œè¯·ç¡®ä¿ç­”æ¡ˆæ¥è‡ªæ­£ç¡®çš„æ–‡æ¡£
3. å¦‚æœé—®é¢˜æ²¡æœ‰æŒ‡å®šå…·ä½“æ–‡æ¡£ï¼Œè¯·åŸºäºæ‰€æœ‰ç›¸å…³ä¿¡æ¯ç»™å‡ºç»¼åˆå›ç­”
4. ç­”æ¡ˆè¦å…·ä½“ã€å‡†ç¡®ï¼ŒåŒ…å«å…³é”®æ•°æ®
5. ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶è¯´æ˜ä¿¡æ¯æ¥æºï¼ˆå¦‚"æ ¹æ®æ–‡æ¡£ç¬¬Xé¡µ"ï¼‰

è¯·å¼€å§‹å›ç­”ï¼š
"""
    
    return prompt

def generate_ai_answer(prompt: str) -> str:
    """
    è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
    """
    try:
        # è°ƒç”¨æå®¢æ™ºåŠAPIç”Ÿæˆç­”æ¡ˆ
        headers = {
            "Authorization": f"Bearer {GEEKAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-4o-mini",  # ä½¿ç”¨åˆé€‚çš„æ¨¡å‹
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æ–‡æ¡£ä¿¡æ¯å‡†ç¡®å›ç­”é—®é¢˜ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.3
        }
        
        response = requests.post(
            GEEKAI_CHAT_URL,
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                answer = result["choices"][0]["message"]["content"]
                return answer
            else:
                logger.error(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°æ ¼å¼é”™è¯¯ï¼Œè¯·é‡è¯•ã€‚"
        else:
            logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}, {response.text}")
            return f"æŠ±æ­‰ï¼ŒAPIè°ƒç”¨å¤±è´¥ï¼ˆçŠ¶æ€ç ï¼š{response.status_code}ï¼‰ï¼Œè¯·é‡è¯•ã€‚"
            
    except requests.exceptions.Timeout:
        logger.error("APIè°ƒç”¨è¶…æ—¶")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆè¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚"
    except requests.exceptions.RequestException as e:
        logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
        return f"æŠ±æ­‰ï¼ŒAPIè¯·æ±‚å¼‚å¸¸ï¼š{str(e)}ï¼Œè¯·é‡è¯•ã€‚"
    except Exception as e:
        logger.error(f"ç”ŸæˆAIç­”æ¡ˆå¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}ï¼Œè¯·é‡è¯•ã€‚"

def extract_keywords_from_content(content: str) -> List[str]:
    """ä»å†…å®¹æå–å…³é”®æ ‡è¯†è¯ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰"""
    keywords = []
    content_lower = content.lower()
    
    # é€šç”¨å…³é”®è¯
    general_keywords = ["æŠ•èµ„", "ç®¡ç†", "é£é™©", "æ”¶ç›Š", "è´¹ç”¨", "æ—¥æœŸ", "ç›®æ ‡", "ç­–ç•¥", "æŠ¥å‘Š", "åˆ†æ"]
    for keyword in general_keywords:
        if keyword in content_lower:
            keywords.append(keyword)
    
    # æ–‡æ¡£ç»“æ„å…³é”®è¯
    structure_keywords = ["æ ‡é¢˜", "ç« èŠ‚", "è¡¨æ ¼", "å›¾è¡¨", "é™„å½•", "æ‘˜è¦", "ç»“è®º"]
    for keyword in structure_keywords:
        if keyword in content_lower:
            keywords.append(keyword)
    
    return keywords[:8]  # é™åˆ¶å…³é”®è¯æ•°é‡

@app.get("/api/health")
def health_check():
    """
    å¥åº·æ£€æŸ¥
    """
    try:
        # æ£€æŸ¥ESè¿æ¥
        es_info = es_client.info()
        
        # æ£€æŸ¥PyMuPDFå¯ç”¨æ€§
        pymupdf_status = "available" if PYMUPDF_AVAILABLE else "unavailable"
        
        # æ£€æŸ¥æå®¢æ™ºåŠAPI
        try:
            headers = {"Authorization": f"Bearer {GEEKAI_API_KEY}"}
            response = requests.get("https://geekai.co/api/v1/models", headers=headers, timeout=5)
            geekai_status = "available" if response.status_code == 200 else "unavailable"
        except:
            geekai_status = "unavailable"
        
        return {
            "status": "healthy",
            "elasticsearch": "connected",
            "pymupdf": pymupdf_status,
            "geekai_api": geekai_status,
            "timestamp": "2024-01-01T00:00:00Z"
        }
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
