#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ - ä¸»åº”ç”¨
æ•´åˆPyMuPDF Pro + PyMuPDF4LLM + LangChain + æå®¢æ™ºåŠAPI
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import json
import logging
import os
import tempfile
from pathlib import Path
import requests

# PyMuPDFç›¸å…³
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("âŒ PyMuPDF ä¸å¯ç”¨")

# LangChainç›¸å…³
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.schema import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âŒ LangChain ä¸å¯ç”¨")

# ESç›¸å…³
from elasticsearch import Elasticsearch
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ", version="2.0.0")

# å¯¼å…¥é…ç½®
from config import (
    ES_CONFIG, DOCUMENT_CONFIG, EMBEDDING_CONFIG, RAG_CONFIG,
    CHUNKING_CONFIG, GEEKAI_API_KEY, GEEKAI_CHAT_URL,
    GEEKAI_EMBEDDING_URL, DEFAULT_EMBEDDING_MODEL
)

# è¾…åŠ©ï¼šæ„å»ºé¡µæ–‡æœ¬ä¸è¯çº§ç´¢å¼•ï¼ˆç”¨äºbboxå®šä½ï¼‰
def build_page_text_and_word_index(page: "fitz.Page") -> (str, list):
    """è¿”å› (page_text, word_entries), å…¶ä¸­ word_entries ä¸º [ (start, end, (x0,y0,x1,y1)) ]."""
    try:
        words = page.get_text("words")  # (x0,y0,x1,y1,word,block_no,line_no,word_no)
        # æ’åºï¼šå—â†’è¡Œâ†’è¯
        words_sorted = sorted(words, key=lambda w: (int(w[5]), int(w[6]), int(w[7])))
        parts = []
        entries = []
        last_block, last_line = None, None
        current_pos = 0
        for w in words_sorted:
            x0, y0, x1, y1, token, bno, lno, wno = w
            key = (bno, lno)
            if last_block is None:
                # é¦–è¯ï¼Œç›´æ¥å†™
                parts.append(token)
                start = current_pos
                end = start + len(token)
                entries.append((start, end, (float(x0), float(y0), float(x1), float(y1))))
                current_pos = end
            else:
                if key != (last_block, last_line):
                    # æ¢è¡Œ
                    parts.append("\n")
                    current_pos += 1
                    parts.append(token)
                    start = current_pos
                    end = start + len(token)
                    entries.append((start, end, (float(x0), float(y0), float(x1), float(y1))))
                    current_pos = end
                else:
                    # åŒè¡Œè¯ï¼Œç©ºæ ¼åˆ†éš”
                    parts.append(" ")
                    current_pos += 1
                    parts.append(token)
                    start = current_pos
                    end = start + len(token)
                    entries.append((start, end, (float(x0), float(y0), float(x1), float(y1))))
                    current_pos = end
            last_block, last_line = key
        page_text = "".join(parts)
        return page_text, entries
    except Exception:
        txt = page.get_text() or ""
        return txt, []

def compute_bbox_union_for_range(entries: list, start: int, end: int) -> list:
    """æ ¹æ®è¯çº§ç´¢å¼•åˆ—è¡¨ï¼Œè®¡ç®—è¦†ç›– [start,end) çš„bboxå¹¶é›†ï¼Œè¿”å› [x0,y0,x1,y1]ï¼Œæ— åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚"""
    x0 = y0 = float("inf")
    x1 = y1 = float("-inf")
    found = False
    for (s, e, (a, b, c, d)) in entries:
        if e <= start or s >= end:
            continue
        found = True
        x0 = min(x0, a)
        y0 = min(y0, b)
        x1 = max(x1, c)
        y1 = max(y1, d)
    return [x0, y0, x1, y1] if found else []

# åˆå§‹åŒ–ESå®¢æˆ·ç«¯
es_client = Elasticsearch(
    [f"http://{ES_CONFIG['host']}:{ES_CONFIG['port']}"],
    basic_auth=(ES_CONFIG['username'], ES_CONFIG['password']) if ES_CONFIG['username'] else None,
    verify_certs=ES_CONFIG['verify_certs']
)

# æå®¢API embeddingå‡½æ•°
def get_embedding(text: str) -> list:
    """ä½¿ç”¨æå®¢APIè·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
    try:
        url = GEEKAI_EMBEDDING_URL
        headers = {
            "Authorization": f"Bearer {GEEKAI_API_KEY}",
            "Content-Type": "application/json"
        }
        data = {
            "model": DEFAULT_EMBEDDING_MODEL,
            "input": [text],
            "intent": "search_document"
        }
        response = requests.post(url, headers=headers, json=data, timeout=20)
        if response.status_code == 200:
            return response.json().get("data", [{}])[0].get("embedding")
        else:
            logger.error(f"æå®¢API embeddingå¤±è´¥: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        logger.error(f"æå®¢API embeddingå¼‚å¸¸: {e}")
        return None

# è¯·æ±‚æ¨¡å‹
class LdapValidateRequest(BaseModel):
    username: str
    password: str

class ChatRequest(BaseModel):
    question: str
    user_id: str

class DocumentProcessRequest(BaseModel):
    knowledge_id: int
    knowledge_name: str
    description: str
    tags: List[str]
    effective_time: str

# å“åº”æ¨¡å‹
class LdapValidateResponse(BaseModel):
    success: bool
    message: str
    email: Optional[str] = None
    role: Optional[str] = None

class KnowledgeReference(BaseModel):
    knowledge_id: int
    knowledge_name: str
    description: str
    tags: List[str]
    effective_time: str
    attachments: List[str]
    relevance: float
    # è¿½æº¯å®šä½å¢å¼º
    source_file: Optional[str] = None
    page_num: Optional[int] = None
    chunk_index: Optional[int] = None
    chunk_type: Optional[str] = None
    # æ–°å¢ï¼šè¿”å›å—åæ ‡ä¸å­—ç¬¦èŒƒå›´ï¼Œä¾¿äºå‰ç«¯é«˜äº®
    bbox_union: Optional[List[float]] = None
    char_start: Optional[int] = None
    char_end: Optional[int] = None

class ChatResponse(BaseModel):
    answer: str
    references: List[KnowledgeReference]
    session_id: Optional[str] = None

class DocumentProcessResponse(BaseModel):
    success: bool
    message: str
    chunks_count: int
    knowledge_id: int

@app.get("/")
def read_root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ",
        "version": "2.0.0",
        "features": [
            "PyMuPDF Pro æ–‡æ¡£å¤„ç†",
            "PyMuPDF4LLM ç»“æ„åŒ–åˆ†å—", 
            "LangChain å‘é‡åŒ–",
            "Elasticsearch å­˜å‚¨",
            "æå®¢æ™ºåŠAPI æ™ºèƒ½é—®ç­”"
        ]
    }

@app.post("/api/ldap/validate", response_model=LdapValidateResponse)
def validate_ldap_user(request: LdapValidateRequest):
    """LDAPç”¨æˆ·éªŒè¯ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    logger.info(f"LDAPéªŒè¯è¯·æ±‚: {request.username}")
    
    # æ¨¡æ‹ŸLDAPéªŒè¯
    if request.username == "admin" and request.password == "password":
        return LdapValidateResponse(
            success=True,
            message="éªŒè¯æˆåŠŸ",
            email="admin@example.com",
            role="admin"
        )
    else:
        return LdapValidateResponse(
            success=False,
            message="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"
        )

def process_document_unified(file_path: str, filename: str, knowledge_id: int) -> Dict:
    """
    ç»Ÿä¸€å¤„ç†æ–‡æ¡£ï¼Œç›´æ¥ä½¿ç”¨PyMuPDFçš„å—çº§ä½ç½®ä¿¡æ¯
    """
    try:
        # æ‰“å¼€æ–‡æ¡£
        if filename.lower().endswith('.pdf'):
            doc = fitz.open(file_path)
            try:
                logger.info(f"æˆåŠŸæ‰“å¼€æ–‡æ¡£ï¼Œé¡µæ•°: {len(doc)}")
                
                # ç›´æ¥ä½¿ç”¨PyMuPDFçš„å—çº§ä½ç½®ä¿¡æ¯
                documents = extract_documents_with_block_positions(doc, filename)
                
                # åˆå¹¶æ‰€æœ‰æ–‡æ¡£å†…å®¹ï¼Œå‡†å¤‡ç”¨LangChainåˆ†å‰²
                all_content = ""
                all_positions = []
                
                for doc_info in documents:
                    all_content += doc_info["content"] + "\n"
                    all_positions.extend(doc_info["positions"])
                
                logger.info(f"åˆå¹¶åæ€»å†…å®¹é•¿åº¦: {len(all_content)} å­—ç¬¦")
                logger.info(f"åˆå¹¶åæ€»ä½ç½®ä¿¡æ¯æ•°é‡: {len(all_positions)}")
                
                # ä½¿ç”¨LangChainåˆ†å‰²å™¨åˆ†å‰²åˆå¹¶åçš„å†…å®¹
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200,
                    length_function=len,
                )
                
                # åˆ†å‰²æ–‡æœ¬å†…å®¹
                text_chunks = text_splitter.split_text(all_content)
                logger.info(f"LangChainåˆ†å‰²åç”Ÿæˆ {len(text_chunks)} ä¸ªchunks")
                
                # ä¸ºæ¯ä¸ªchunkåˆ†é…ä½ç½®ä¿¡æ¯
                chunks = []
                for chunk_idx, chunk_text in enumerate(text_chunks):
                    # ä¸ºæ¯ä¸ªchunkåˆ†é…ç›¸å…³çš„ä½ç½®ä¿¡æ¯
                    chunk_positions = assign_positions_to_chunk(chunk_text, all_positions)

                    # è®¡ç®—ä¸»é¡µé¢
                    page_counts: Dict[int, int] = {}
                    for p in chunk_positions:
                        pg = int(p.get('page', 1))
                        page_counts[pg] = page_counts.get(pg, 0) + 1
                    main_page = max(page_counts.items(), key=lambda kv: kv[1])[0] if page_counts else 1
                    
                    # åˆ›å»ºLangChain Document
                    chunk = Document(
                        page_content=chunk_text,
                        metadata={
                            "knowledge_id": knowledge_id,
                            "source_file": filename,
                            "page_num": main_page,
                            "chunk_index": chunk_idx,
                            "positions": chunk_positions,
                            "bbox": calculate_chunk_bbox(chunk_positions),
                            "document_name": filename,
                            "document_type": "æ–‡æ¡£",
                            "keywords": extract_keywords_from_content(chunk_text)
                        }
                    )
                    chunks.append(chunk)
            finally:
                try:
                    doc.close()
                except Exception:
                    pass
            
        else:
            # å¤„ç†å…¶ä»–ç±»å‹æ–‡æ¡£
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
            )
            
            text_chunks = text_splitter.split_text(content)
            chunks = []
            
            for chunk_idx, chunk_text in enumerate(text_chunks):
                chunk = Document(
                    page_content=chunk_text,
                    metadata={
                        "knowledge_id": knowledge_id,
                        "source_file": filename,
                        "page_num": 1,
                        "chunk_index": chunk_idx,
                        "document_name": filename,
                        "document_type": "æ–‡æ¡£",
                        "keywords": extract_keywords_from_content(chunk_text)
                    }
                )
                chunks.append(chunk)
        
        logger.info(f"æœ€ç»ˆç”Ÿæˆ {len(chunks)} ä¸ªchunks")
        
        # å­˜å‚¨åˆ°ES
        stored_count = store_chunks_to_es(chunks, knowledge_id)
        
        return {
            "chunks_count": stored_count,
            "total_chunks": len(chunks),
            "success": stored_count > 0
        }
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise e

def extract_documents_with_block_positions(doc, filename: str) -> List[Dict]:
    """
    ç›´æ¥ä»PyMuPDFæå–æ–‡æ¡£å—å’Œä½ç½®ä¿¡æ¯
    """
    documents = []

    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        blocks = page.get_text("dict")

        for block_idx, block in enumerate(blocks["blocks"]):
            if "lines" in block:
                # æ”¶é›†æ•´ä¸ªå—çš„æ‰€æœ‰æ–‡æœ¬å’Œä½ç½®ä¿¡æ¯
                block_text = ""
                block_positions = []

                for line_idx, line in enumerate(block["lines"]):
                    for span_idx, span in enumerate(line["spans"]):
                        text = span["text"].strip()
                        if text:
                            block_text += text + " "
                            block_positions.append({
                                "text": text,
                                "bbox": span["bbox"],
                                "font_size": span["size"],
                                "font": span["font"],
                                "span_idx": span_idx,
                                "line_idx": line_idx,
                                "page": page_num + 1,
                            })

                if block_text.strip():
                    # è®¡ç®—æ•´ä¸ªå—çš„è¾¹ç•Œæ¡†ï¼ˆå†…è”å®ç°ï¼‰
                    if block_positions:
                        bx0 = min(p["bbox"][0] for p in block_positions)
                        by0 = min(p["bbox"][1] for p in block_positions)
                        bx1 = max(p["bbox"][2] for p in block_positions)
                        by1 = max(p["bbox"][3] for p in block_positions)
                        block_bbox = [bx0, by0, bx1, by1]
                    else:
                        block_bbox = [0, 0, 0, 0]

                    documents.append({
                        "content": block_text.strip(),
                        "page": page_num + 1,
                        "block_idx": block_idx,
                        "positions": block_positions,
                        "bbox": block_bbox
                    })

    return documents


def assign_positions_to_chunk(chunk_text: str, positions: List[Dict]) -> List[Dict]:
    """
    ä¸ºchunkåˆ†é…å¯¹åº”çš„ä½ç½®ä¿¡æ¯
    ä½¿ç”¨ç®€å•çš„æ–‡æœ¬åŒ…å«å…³ç³»ï¼Œè€Œä¸æ˜¯å¤æ‚çš„ç›¸ä¼¼åº¦è®¡ç®—
    """
    chunk_positions = []

    for pos in positions:
        pos_text = pos["text"]
        # å¦‚æœchunkåŒ…å«è¿™ä¸ªä½ç½®çš„æ–‡æœ¬ï¼Œå°±åˆ†é…ç»™å®ƒ
        if pos_text in chunk_text:
            chunk_positions.append(pos)

    return chunk_positions

def calculate_chunk_bbox(positions: List[Dict]) -> List[float]:
    """è®¡ç®—chunkçš„è¾¹ç•Œæ¡†ï¼šå…ˆæŒ‰spanæ‰€åœ¨é¡µèšç±»ï¼Œå–ä½ç½®æœ€å¤šçš„é¡µä½œä¸ºä¸»é¡µé¢ï¼Œå†å¯¹è¯¥é¡µçš„positionsæ±‚å¹¶é›†"""
    if not positions:
        return [0, 0, 0, 0]

    # ä½ç½®é‡Œéœ€è¦å¸¦ä¸Špageä¿¡æ¯ï¼›è‹¥æ²¡æœ‰ï¼Œé»˜è®¤é¡µ=1
    page_to_positions: Dict[int, List[Dict]] = {}
    for pos in positions:
        page = int(pos.get('page', 1)) if isinstance(pos, dict) else 1
        page_to_positions.setdefault(page, []).append(pos)

    # é€‰æ‹©ä½ç½®æœ€å¤šçš„é¡µä½œä¸ºä¸»é¡µé¢
    main_page = max(page_to_positions.items(), key=lambda kv: len(kv[1]))[0]
    main_positions = page_to_positions[main_page]

    x0 = min(p["bbox"][0] for p in main_positions)
    y0 = min(p["bbox"][1] for p in main_positions)
    x1 = max(p["bbox"][2] for p in main_positions)
    y1 = max(p["bbox"][3] for p in main_positions)

    return [x0, y0, x1, y1]

def generate_pdfllm_style_markdown(doc, filename: str) -> tuple[str, List[Dict]]:
    """
    ä½¿ç”¨PyMuPDFç”Ÿæˆå¹²å‡€çš„Markdownå†…å®¹å’Œç²¾ç¡®çš„æ–‡æœ¬-åæ ‡æ˜ å°„
    è¿”å›: (markdown_content, position_mapping)
    """
    content_lines = []
    position_mapping = []
    
    # æ·»åŠ æ–‡æ¡£æ ‡é¢˜
    content_lines.append(f"# {filename}")
    content_lines.append("")
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        
        # é¡µé¢æ ‡é¢˜
        content_lines.append(f"## ç¬¬ {page_num + 1} é¡µ")
        content_lines.append("")
        
        # è·å–é¡µé¢æ–‡æœ¬å—
        blocks = page.get_text("dict")
        
        if "blocks" in blocks:
            for block_idx, block in enumerate(blocks["blocks"]):
                if "lines" in block:
                    block_text = ""
                    block_positions = []
                    
                    for line_idx, line in enumerate(block["lines"]):
                        line_text = ""
                        line_positions = []
                        
                        for span_idx, span in enumerate(line["spans"]):
                            text = span["text"].strip()
                            if text:
                                # åªæ·»åŠ æ–‡æœ¬å†…å®¹ï¼Œä¸æ·»åŠ ä½ç½®æ ‡ç­¾
                                line_text += text + " "
                                
                                # è®°å½•ä½ç½®ä¿¡æ¯
                                line_positions.append({
                                    "text": text,
                                    "page": page_num + 1,
                                    "bbox": span["bbox"],
                                    "font_size": span["size"],
                                    "font": span["font"],
                                    "block_idx": block_idx,
                                    "line_idx": line_idx,
                                    "span_idx": span_idx,
                                    "char_start": len("".join(content_lines)),  # åœ¨æœ€ç»ˆæ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®
                                    "char_end": len("".join(content_lines)) + len(text)  # åœ¨æœ€ç»ˆæ–‡æœ¬ä¸­çš„ç»“æŸä½ç½®
                                })
                        
                        if line_text.strip():
                            block_text += line_text.strip() + "\n"
                            block_positions.extend(line_positions)
                    
                    if block_text.strip():
                        # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯æ ‡é¢˜ï¼ˆåŸºäºå­—ä½“å¤§å°ï¼‰
                        max_font_size = max(span["size"] for line in block["lines"] for span in line["spans"])
                        if max_font_size > 12:  # å‡è®¾å¤§äº12ptçš„æ˜¯æ ‡é¢˜
                            content_lines.append(f"### {block_text.strip()}")
                        else:
                            content_lines.append(block_text.strip())
                        content_lines.append("")
                        
                        # æ·»åŠ ä½ç½®ä¿¡æ¯åˆ°æ˜ å°„
                        position_mapping.extend(block_positions)
    
    # åˆå¹¶æ‰€æœ‰å†…å®¹
    markdown_content = "\n".join(content_lines)
    
    # æ›´æ–°å­—ç¬¦ä½ç½®ï¼ˆå› ä¸ºæ¢è¡Œç¬¦ç­‰ä¼šå½±å“ä½ç½®ï¼‰
    current_pos = 0
    for pos_info in position_mapping:
        text = pos_info["text"]
        pos_info["char_start"] = current_pos
        pos_info["char_end"] = current_pos + len(text)
        current_pos += len(text) + 1  # +1 for space
    
    return markdown_content, position_mapping

def extract_position_mapping(doc) -> List[Dict]:
    """
    å•ç‹¬æå–ä½ç½®ä¿¡æ¯æ˜ å°„ï¼Œä¸åµŒå…¥åˆ°æ–‡æœ¬å†…å®¹ä¸­
    """
    position_mapping = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        blocks = page.get_text("dict")
        
        if "blocks" in blocks:
            for block_idx, block in enumerate(blocks["blocks"]):
                if "lines" in block:
                    for line_idx, line in enumerate(block["lines"]):
                        for span_idx, span in enumerate(line["spans"]):
                            text = span["text"].strip()
                            if text:
                                position_mapping.append({
                                    "text": text,
                                    "page": page_num + 1,
                                    "bbox": span["bbox"],
                                    "font_size": span["size"],
                                    "font": span["font"],
                                    "block_idx": block_idx,
                                    "line_idx": line_idx,
                                    "span_idx": span_idx
                                })
    
    return position_mapping

def find_best_position_match(chunk_text: str, position_mapping: List[Dict]) -> Optional[Dict]:
    """
    ä¸ºchunkæ‰¾åˆ°æœ€åŒ¹é…çš„ä½ç½®ä¿¡æ¯
    ä½¿ç”¨å—çº§æ¨¡ç³ŠåŒ¹é…ï¼Œè€Œä¸æ˜¯ç²¾ç¡®çš„æ–‡æœ¬ç‰‡æ®µåŒ¹é…
    """
    if not position_mapping:
        return None
    
    chunk_text_lower = chunk_text.lower()
    chunk_words = set(chunk_text_lower.split())
    
    # å®šä¹‰å…³é”®ä¿¡æ¯å…³é”®è¯å’Œå¯¹åº”çš„ä¼˜å…ˆçº§
    key_phrases_priority = [
        # é«˜ä¼˜å…ˆçº§ï¼šåŸºé‡‘æ ¸å¿ƒä¿¡æ¯
        (["åŸºé‡‘æ€»å€¼", "4.4377", "äº¿ç¾å…ƒ"], 10),
        (["åŸºé‡‘ä»·æ ¼", "èµ„äº§å‡€å€¼", "5.7741"], 9),
        (["æˆç«‹æ—¥æœŸ", "2010å¹´8æœˆ2æ—¥"], 8),
        (["åŸºé‡‘ç»ç†", "Justin Kass", "David Oberto", "Michael Yee"], 7),
        (["ç®¡ç†è´¹", "1.19%"], 6),
        (["æŠ•èµ„ç›®æ ‡", "ç¾å›½å€ºåˆ¸", "é«˜æ”¶ç›Š"], 5),
        (["æ”¶ç›Šåˆ†é…", "æ¯æœˆ"], 4),
        (["è´¢æ”¿å¹´åº¦", "9æœˆ30æ—¥"], 3),
        (["äº¤æ˜“æ—¥", "æ¯æ—¥"], 2),
        (["æŠ•èµ„ç»ç†", "å®‰è”æŠ•èµ„"], 1)
    ]
    
    # ç¬¬ä¸€è½®ï¼šä¼˜å…ˆåŒ¹é…åŒ…å«å…³é”®ä¿¡æ¯çš„æ–‡æœ¬
    for pos_info in position_mapping:
        text = pos_info.get("text", "").strip()
        if text:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜ä¼˜å…ˆçº§çš„å…³é”®ä¿¡æ¯
            for key_phrases, priority in key_phrases_priority:
                if any(keyword in text for keyword in key_phrases):
                    # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
                    text_lower = text.lower()
                    chunk_lower = chunk_text_lower
                    
                    # ä½¿ç”¨å­—ç¬¦é‡å åº¦è®¡ç®—ç›¸ä¼¼åº¦
                    overlap = sum(1 for c in text_lower if c in chunk_lower)
                    base_score = overlap / max(len(text_lower), 1)
                    
                    # åº”ç”¨ä¼˜å…ˆçº§æƒé‡
                    weighted_score = base_score * priority
                    
                    if weighted_score > best_score:
                        best_score = weighted_score
                        best_match = pos_info
                    break  # æ‰¾åˆ°åŒ¹é…çš„å…³é”®è¯å°±è·³å‡ºå†…å±‚å¾ªç¯
    
    # ç¬¬äºŒè½®ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®ä¿¡æ¯ï¼Œå°è¯•ç²¾ç¡®åŒ…å«åŒ¹é…
    if not best_match:
        for pos_info in position_mapping:
            text = pos_info.get("text", "").strip()
            if text and text.lower() in chunk_text_lower:
                # è®¡ç®—åŒ¹é…åº¦ï¼šæ–‡æœ¬é•¿åº¦ä¸chunké•¿åº¦çš„æ¯”ä¾‹
                score = len(text) / max(len(chunk_text), 1)
                if score > best_score:
                    best_score = score
                    best_match = pos_info
    
    # ç¬¬ä¸‰è½®ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å•è¯çº§åˆ«çš„åŒ¹é…
    if not best_match:
        for pos_info in position_mapping:
            text = pos_info.get("text", "").strip()
            if text:
                text_words = set(text.lower().split())
                # è®¡ç®—å•è¯é‡å åº¦
                overlap = len(chunk_words.intersection(text_words))
                if overlap > 0:
                    score = overlap / max(len(chunk_words), 1)
                    if score > best_score:
                        best_score = score
                        best_match = pos_info
    
    # ç¬¬å››è½®ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ä½ç½®ä¿¡æ¯
    if not best_match and position_mapping:
        for pos_info in position_mapping:
            if pos_info.get("bbox") and len(pos_info.get("bbox", [])) == 4:
                return pos_info
    
    return best_match

def expand_bbox_to_block_level(bbox: List[float], page_width: float, page_height: float) -> List[float]:
    """
    å°†ç²¾ç¡®çš„æ–‡æœ¬ç‰‡æ®µbboxæ‰©å±•ä¸ºå—çº§bboxï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
    ä½¿ç”¨æ›´ä¿å®ˆçš„æ‰©å±•ç­–ç•¥ï¼Œé¿å…è¿‡åº¦æ‰©å±•
    """
    if len(bbox) != 4:
        return bbox
    
    x0, y0, x1, y1 = bbox
    
    # è®¡ç®—å½“å‰æ–‡æœ¬çš„å®½åº¦å’Œé«˜åº¦
    text_width = x1 - x0
    text_height = y1 - y0
    
    # æ›´ä¿å®ˆçš„æ‰©å±•ç­–ç•¥
    # æ°´å¹³æ‰©å±•ï¼šå·¦å³å„æ‰©å±•æ–‡æœ¬å®½åº¦çš„50%ï¼Œä½†ä¸è¶…è¿‡é¡µé¢è¾¹è·
    horizontal_expansion = min(text_width * 0.5, 30)  # æœ€å¤§æ‰©å±•30åƒç´ 
    
    expanded_x0 = max(20, x0 - horizontal_expansion)
    expanded_x1 = min(page_width - 20, x1 + horizontal_expansion)
    
    # å‚ç›´æ‰©å±•ï¼šä¸Šä¸‹å„æ‰©å±•æ–‡æœ¬é«˜åº¦çš„50%ï¼Œä½†ä¸è¶…è¿‡é¡µé¢è¾¹è·
    vertical_expansion = min(text_height * 0.5, 20)  # æœ€å¤§æ‰©å±•20åƒç´ 
    
    expanded_y0 = max(20, y0 - vertical_expansion)
    expanded_y1 = min(page_height - 20, y1 + vertical_expansion)
    
    return [expanded_x0, expanded_y0, expanded_x1, expanded_y1]

def find_best_position_match_block_level(chunk_text: str, position_mapping: List[Dict], page_width: float, page_height: float) -> Optional[Dict]:
    """
    ä¸ºchunkæ‰¾åˆ°æœ€åŒ¹é…çš„ä½ç½®ä¿¡æ¯ï¼Œå¹¶æ‰©å±•ä¸ºå—çº§åæ ‡
    """
    # å…ˆæ‰¾åˆ°æœ€ä½³åŒ¹é…
    best_match = find_best_position_match(chunk_text, position_mapping)
    
    if best_match and best_match.get("bbox"):
        # æ‰©å±•bboxä¸ºå—çº§åæ ‡
        expanded_bbox = expand_bbox_to_block_level(best_match["bbox"], page_width, page_height)
        
        # åˆ›å»ºæ–°çš„ä½ç½®ä¿¡æ¯ï¼ŒåŒ…å«æ‰©å±•åçš„åæ ‡
        block_level_match = best_match.copy()
        block_level_match["bbox"] = expanded_bbox
        block_level_match["bbox_type"] = "block_level"  # æ ‡è®°ä¸ºå—çº§åæ ‡
        
        return block_level_match
    
    return best_match

def store_chunks_to_es(chunks: List[Document], knowledge_id: int):
    """
    å°†chunkså­˜å‚¨åˆ°Elasticsearch
    """
    stored_count = 0
    
    for i, chunk in enumerate(chunks):
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = hashlib.md5(f"{knowledge_id}_{i}_{chunk.page_content[:100]}".encode()).hexdigest()
            
            # ä¸ºchunkç”Ÿæˆembedding
            chunk_embedding = get_embedding(chunk.page_content)
            if not chunk_embedding:
                logger.warning(f"Chunk {i} embeddingç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            # å‡†å¤‡ESæ–‡æ¡£
            es_doc = {
                "content": chunk.page_content,
                "embedding": chunk_embedding,
                "knowledge_id": chunk.metadata.get("knowledge_id", knowledge_id),
                "knowledge_name": chunk.metadata.get("knowledge_name", ""),
                "description": chunk.metadata.get("description", ""),
                "tags": chunk.metadata.get("tags", ""),
                "effective_time": chunk.metadata.get("effective_time", ""),
                "source_file": chunk.metadata.get("source_file", ""),
                "chunk_index": chunk.metadata.get("chunk_index", i),
                "chunk_type": chunk.metadata.get("chunk_type", "content"),
                "page_num": chunk.metadata.get("page_num", 1),
                "bbox": chunk.metadata.get("bbox", []),
                "positions": chunk.metadata.get("positions", []),
                "weight": 1.0
            }
            
            # å­˜å‚¨åˆ°ES
            es_client.index(index=ES_CONFIG['index'], id=doc_id, document=es_doc)
            stored_count += 1
            
            if (i + 1) % 10 == 0:
                logger.info(f"å·²å­˜å‚¨ {i + 1}/{len(chunks)} ä¸ªchunks")
                
        except Exception as e:
            logger.error(f"å­˜å‚¨chunk {i} å¤±è´¥: {e}")
            continue

    logger.info(f"ESå­˜å‚¨å®Œæˆï¼ŒæˆåŠŸå­˜å‚¨ {stored_count}/{len(chunks)} ä¸ªchunks")
    return stored_count

@app.post("/api/document/process", response_model=DocumentProcessResponse)
async def process_document(
    file: UploadFile = File(...),
    knowledge_id: int = Form(None),
    knowledge_name: str = Form(None),
    description: str = Form(None),
    tags: str = Form(None),
    effective_time: str = Form(None)
):
    """
    ä½¿ç”¨ PyMuPDF Pro + PyMuPDF4LLM ç»Ÿä¸€å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£
    æ”¯æŒ PDFã€Wordã€Excelã€PowerPointã€TXT ç­‰æ ¼å¼
    """
    logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file.filename}, çŸ¥è¯†ID: {knowledge_id}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        file_extension = Path(file.filename).suffix.lower()
        
        # PyMuPDF Pro æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {
            ".pdf", ".docx", ".doc", ".xlsx", ".xls", 
            ".pptx", ".ppt", ".txt", ".hwp", ".hwpx"
        }
        
        if file_extension not in supported_extensions:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}")
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # å¤„ç†æ–‡æ¡£
            result = process_document_unified(
                temp_file_path, file.filename, knowledge_id
            )
            
            return DocumentProcessResponse(
                success=True,
                message=f"æ–‡æ¡£å¤„ç†æˆåŠŸ: {file.filename}",
                chunks_count=result["chunks_count"],
                knowledge_id=int(knowledge_id) if knowledge_id is not None else 0
            )
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file_path)
            
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

@app.post("/api/rag/chat", response_model=ChatResponse)
def chat_with_rag(request: ChatRequest):
    """
    åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”
    """
    logger.info(f"RAGèŠå¤©è¯·æ±‚: {request.question}")
    
    try:
        # 1. å‘é‡æœç´¢æ‰¾åˆ°ç›¸å…³chunks
        question_embedding = get_embedding(request.question)
        if not question_embedding:
            return ChatResponse(
                answer="æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆé—®é¢˜çš„å‘é‡è¡¨ç¤ºï¼Œè¯·é‡è¯•ã€‚",
                references=[],
                session_id=request.user_id
            )
        
        # ç®€åŒ–æœç´¢é€»è¾‘ - ç›´æ¥è¿”å›è¯­ä¹‰ç›¸ä¼¼åº¦æœ€é«˜çš„chunks
        search_query = {
            "size": 5,  # è¿”å›å‰5ä¸ªæœ€ç›¸å…³çš„chunks
            "query": {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": question_embedding}
                    }
                }
            },
            "_source": ["content", "metadata", "knowledge_id", "knowledge_name", "source_file", 
                       "page_num", "chunk_index", "bbox", "positions"]
        }
        
        search_response = es_client.search(index=ES_CONFIG['index'], body=search_query)
        hits = search_response.get('hits', {}).get('hits', [])
        
        if not hits:
            return ChatResponse(
                answer="æŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                references=[],
                session_id=request.user_id
            )
        
        # 2. æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_chunks = []
        for hit in hits:
            source = hit['_source']
            score = hit['_score']
            
            # æ„å»ºæ¯ä¸ªchunkçš„å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯
            chunk_info = {
                "content": source.get('content', ''),
                "metadata": {
                    "document_name": source.get('source_file', 'N/A'),
                    "document_type": "æ–‡æ¡£",  # é€šç”¨æ–‡æ¡£ç±»å‹
                    "page_num": source.get('page_num', 'N/A'),
                    "chunk_index": source.get('chunk_index', 'N/A'),
                    "bbox": source.get('bbox', []),
                    "positions": source.get('positions', []),
                    "knowledge_name": source.get('knowledge_name', 'N/A'),
                    "relevance_score": round(score, 3)
                }
            }
            context_chunks.append(chunk_info)
        
        # 3. æ„å»ºå¢å¼ºçš„RAGæç¤ºè¯
        enhanced_prompt = build_enhanced_rag_prompt(request.question, context_chunks)
        
        # 4. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
        answer = generate_ai_answer(enhanced_prompt)
        
        # 5. æ„å»ºå¼•ç”¨ä¿¡æ¯
        references = []
        for chunk in context_chunks:
            metadata = chunk['metadata']
            references.append(KnowledgeReference(
                knowledge_id=0,  # è¿™é‡Œéœ€è¦ä»chunkä¸­è·å–
                knowledge_name=metadata.get('knowledge_name', ''),
                description=f"æ–‡æ¡£: {metadata.get('document_name', '')}",
                tags=[metadata.get('document_type', '')],
                effective_time="",
                attachments=[metadata.get('document_name', '')],
                relevance=metadata.get('relevance_score', 0.0),
                source_file=metadata.get('document_name', ''),
                page_num=metadata.get('page_num', 0),
                chunk_index=metadata.get('chunk_index', 0),
                chunk_type="content",
                bbox_union=metadata.get('bbox', []),  # ä½¿ç”¨æ–°çš„bboxå­—æ®µ
                char_start=0,  # ä¸å†ä½¿ç”¨å­—ç¬¦ä½ç½®
                char_end=0
            ))
        
        return ChatResponse(
            answer=answer,
            references=references,
            session_id=request.user_id
        )
        
    except Exception as e:
        logger.error(f"RAGèŠå¤©å¤±è´¥: {e}")
        return ChatResponse(
            answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
            references=[],
            session_id=request.user_id
        )

def build_enhanced_rag_prompt(question: str, context_chunks: List[Dict]) -> str:
    """
    æ„å»ºå¢å¼ºçš„RAGæç¤ºè¯ï¼Œè®©å¤§æ¨¡å‹èƒ½çœ‹åˆ°å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    context_parts = []
    
    for i, chunk in enumerate(context_chunks):
        metadata = chunk['metadata']
        
        # æ„å»ºæ¯ä¸ªchunkçš„è¯¦ç»†ä¸Šä¸‹æ–‡ä¿¡æ¯
        chunk_context = f"""
=== å¼•ç”¨ {i+1} ===
ğŸ“„ æ–‡æ¡£åç§°: {metadata.get('document_name', 'N/A')}
ğŸ“‹ æ–‡æ¡£ç±»å‹: {metadata.get('document_type', 'N/A')}
ğŸ“– é¡µç : {metadata.get('page_num', 'N/A')}
ğŸ”¢ å—åº: {metadata.get('chunk_index', 'N/A')}
ğŸ¯ ç›¸å…³æ€§: {metadata.get('relevance_score', 'N/A')}
ğŸ“ åæ ‡: {metadata.get('bbox', [])}
ğŸ“ å†…å®¹: {chunk.get('content', '')}
"""
        context_parts.append(chunk_context)
    
    # æ„å»ºå®Œæ•´æç¤º
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

æ¯ä¸ªå¼•ç”¨éƒ½åŒ…å«äº†å®Œæ•´çš„æ–‡æ¡£ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æ¡£åç§°å’Œç±»å‹
- é¡µç å’Œå—åº
- ç›¸å…³æ€§è¯„åˆ†å’Œä½ç½®åæ ‡
- å…·ä½“å†…å®¹

è¯·ä»”ç»†åˆ†æè¿™äº›ä¿¡æ¯ï¼Œå¹¶æ ¹æ®é—®é¢˜æ‰¾åˆ°æœ€å‡†ç¡®çš„ç­”æ¡ˆã€‚

é—®é¢˜: {question}

å‚è€ƒä¿¡æ¯:
{''.join(context_parts)}

è¯·æ ¹æ®é—®é¢˜ï¼Œä»ä¸Šè¿°ä¿¡æ¯ä¸­æ‰¾åˆ°æœ€å‡†ç¡®çš„ç­”æ¡ˆã€‚è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦è¯´"è¯·æŸ¥çœ‹å¼•ç”¨ä¿¡æ¯"
2. å¦‚æœé—®é¢˜æ¶‰åŠç‰¹å®šæ–‡æ¡£ï¼Œè¯·ç¡®ä¿ç­”æ¡ˆæ¥è‡ªæ­£ç¡®çš„æ–‡æ¡£
3. å¦‚æœé—®é¢˜æ²¡æœ‰æŒ‡å®šå…·ä½“æ–‡æ¡£ï¼Œè¯·åŸºäºæ‰€æœ‰ç›¸å…³ä¿¡æ¯ç»™å‡ºç»¼åˆå›ç­”
4. ç­”æ¡ˆè¦å…·ä½“ã€å‡†ç¡®ï¼ŒåŒ…å«å…³é”®æ•°æ®
5. ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶è¯´æ˜ä¿¡æ¯æ¥æºï¼ˆå¦‚"æ ¹æ®æ–‡æ¡£ç¬¬Xé¡µ"ï¼‰

è¯·å¼€å§‹å›ç­”ï¼š
"""
    
    return prompt

def generate_ai_answer(prompt: str) -> str:
    """
    è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
    """
    try:
        # è°ƒç”¨æå®¢æ™ºåŠAPIç”Ÿæˆç­”æ¡ˆ
        headers = {
            "Authorization": f"Bearer {GEEKAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-4o-mini",  # ä½¿ç”¨åˆé€‚çš„æ¨¡å‹
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æ–‡æ¡£ä¿¡æ¯å‡†ç¡®å›ç­”é—®é¢˜ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.3
        }
        
        response = requests.post(
            GEEKAI_CHAT_URL,
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                answer = result["choices"][0]["message"]["content"]
                return answer
            else:
                logger.error(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°æ ¼å¼é”™è¯¯ï¼Œè¯·é‡è¯•ã€‚"
        else:
            logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}, {response.text}")
            return f"æŠ±æ­‰ï¼ŒAPIè°ƒç”¨å¤±è´¥ï¼ˆçŠ¶æ€ç ï¼š{response.status_code}ï¼‰ï¼Œè¯·é‡è¯•ã€‚"
            
    except requests.exceptions.Timeout:
        logger.error("APIè°ƒç”¨è¶…æ—¶")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆè¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚"
    except requests.exceptions.RequestException as e:
        logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
        return f"æŠ±æ­‰ï¼ŒAPIè¯·æ±‚å¼‚å¸¸ï¼š{str(e)}ï¼Œè¯·é‡è¯•ã€‚"
    except Exception as e:
        logger.error(f"ç”ŸæˆAIç­”æ¡ˆå¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}ï¼Œè¯·é‡è¯•ã€‚"

def extract_keywords_from_content(content: str) -> List[str]:
    """ä»å†…å®¹æå–å…³é”®æ ‡è¯†è¯ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰"""
    keywords = []
    content_lower = content.lower()
    
    # é€šç”¨å…³é”®è¯
    general_keywords = ["æŠ•èµ„", "ç®¡ç†", "é£é™©", "æ”¶ç›Š", "è´¹ç”¨", "æ—¥æœŸ", "ç›®æ ‡", "ç­–ç•¥", "æŠ¥å‘Š", "åˆ†æ"]
    for keyword in general_keywords:
        if keyword in content_lower:
            keywords.append(keyword)
    
    # æ–‡æ¡£ç»“æ„å…³é”®è¯
    structure_keywords = ["æ ‡é¢˜", "ç« èŠ‚", "è¡¨æ ¼", "å›¾è¡¨", "é™„å½•", "æ‘˜è¦", "ç»“è®º"]
    for keyword in structure_keywords:
        if keyword in content_lower:
            keywords.append(keyword)
    
    return keywords[:8]  # é™åˆ¶å…³é”®è¯æ•°é‡

@app.get("/api/health")
def health_check():
    """
    å¥åº·æ£€æŸ¥
    """
    try:
        # æ£€æŸ¥ESè¿æ¥
        es_info = es_client.info()
        
        # æ£€æŸ¥PyMuPDFå¯ç”¨æ€§
        pymupdf_status = "available" if PYMUPDF_AVAILABLE else "unavailable"
        
        # æ£€æŸ¥æå®¢æ™ºåŠAPI
        try:
            headers = {"Authorization": f"Bearer {GEEKAI_API_KEY}"}
            response = requests.get("https://geekai.co/api/v1/models", headers=headers, timeout=5)
            geekai_status = "available" if response.status_code == 200 else "unavailable"
        except:
            geekai_status = "unavailable"
        
        return {
            "status": "healthy",
            "elasticsearch": "connected",
            "pymupdf": pymupdf_status,
            "geekai_api": geekai_status,
            "timestamp": "2024-01-01T00:00:00Z"
        }
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
