#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ - ä¸»åº”ç”¨
æ•´åˆPyMuPDF Pro + PyMuPDF4LLM + LangChain + æå®¢æ™ºåŠAPI
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import json
import logging
import os
import tempfile
from pathlib import Path
import requests

# PyMuPDF Pro ç»Ÿä¸€æ–‡æ¡£å¤„ç†
from pymupdf_font_fix import setup_pymupdf_pro_environment, test_pymupdf_pro_initialization

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æœ¬åœ°åŒ…
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# ä¸º PyMuPDF Pro å‡†å¤‡ä¸€ä¸ªASCIIå®‰å…¨çš„å­—ä½“ç›®å½•ï¼Œé¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜
SAFE_FONT_DIR = os.path.join(os.path.dirname(__file__), "fonts_tmp")
os.makedirs(SAFE_FONT_DIR, exist_ok=True)

# çº¦æŸ PyMuPDF å­—ä½“ç›¸å…³ç¯å¢ƒï¼Œé¿å…æ‰«æå«ä¸­æ–‡è·¯å¾„çš„ç³»ç»Ÿå­—ä½“ç›®å½•
os.environ['PYMUPDF_FONT_DIR'] = SAFE_FONT_DIR
os.environ['PYMUPDF_SKIP_FONT_INSTALL'] = '1'   # è·³è¿‡å­—ä½“å®‰è£…
os.environ['PYMUPDF_USE_SYSTEM_FONTS'] = '0'    # ä¸ä½¿ç”¨ç³»ç»Ÿå­—ä½“ç›®å½•

# å¯é€‰å¯¼å…¥ PyMuPDF ä¸ Pro æ‰©å±•
try:
    import pymupdf  # ä¸»åº“
    PYMUPDF_AVAILABLE = True
except Exception:
    PYMUPDF_AVAILABLE = False

try:
    import pymupdf.pro  # Pro æ‰©å±•ï¼ˆå¯é€‰ï¼‰
    # ä½¿ç”¨é…ç½®ä¸­çš„è¯•ç”¨å¯†é’¥ï¼Œå¹¶å¼ºåˆ¶ä½¿ç”¨SAFE_FONT_DIRï¼Œç¦ç”¨è‡ªåŠ¨å­—ä½“è·¯å¾„æ£€æµ‹
    try:
        from config import PYMUPDF_PRO_CONFIG
        trial_key = PYMUPDF_PRO_CONFIG.get('trial_key', '') if isinstance(PYMUPDF_PRO_CONFIG, dict) else ''
    except Exception:
        trial_key = ''

    try:
        if trial_key:
            pymupdf.pro.unlock(trial_key, fontpath=SAFE_FONT_DIR, fontpath_auto=False)
        else:
            pymupdf.pro.unlock(fontpath=SAFE_FONT_DIR, fontpath_auto=False)
        PYMUPDF_PRO_AVAILABLE = True
        logging.info("PyMuPDF Pro è§£é”å®Œæˆï¼ˆä½¿ç”¨å®‰å…¨å­—ä½“ç›®å½•ï¼‰")
    except Exception as e:
        PYMUPDF_PRO_AVAILABLE = False
        logging.warning(f"PyMuPDF Pro è§£é”å¤±è´¥ï¼Œå°†ä½¿ç”¨å…è´¹ç‰ˆæœ¬: {e}")
except Exception:
    PYMUPDF_PRO_AVAILABLE = False

# æ–‡æ¡£å¤„ç†ç›¸å…³
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain.schema import Document
import requests
import json

# PyMuPDF4LLM ç”¨äºç»“æ„åŒ–åˆ†å—
try:
    from mypymupdf4llm import LlamaMarkdownReader
    PYMUPDF4LLM_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥ mypymupdf4llm.LlamaMarkdownReader")
except ImportError as e:
    PYMUPDF4LLM_AVAILABLE = False
    print(f"âŒ PyMuPDF4LLM ä¸å¯ç”¨: {e}ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿåˆ†å—")

# å¼•å…¥åŸºäºæœ¬åœ° mypymupdf4llm çš„å®šä½ä¸é¢„è§ˆèƒ½åŠ›
try:
    # ç”Ÿæˆå¸¦ <sub>pos: ...</sub> çš„ Markdown
    from mypymupdf4llm.helpers.pymupdf_rag import to_markdown as to_md_with_pos
    print("âœ… æˆåŠŸå¯¼å…¥ mypymupdf4llm.helpers.pymupdf_rag.to_markdown")
except Exception as e:
    print(f"âŒ å¯¼å…¥ mypymupdf4llm.helpers.pymupdf_rag.to_markdown å¤±è´¥: {e}")
    to_md_with_pos = None

try:
    # è§£æå¸¦ä½ç½®çš„ Markdown â†’ aligned_positions
    from md_pos_to_aligned import parse_md_with_pos, save_aligned
except Exception:
    parse_md_with_pos = save_aligned = None

try:
    # ç”Ÿæˆé¢„è§ˆ PNG
    from preview_alignment import draw_preview
except Exception:
    draw_preview = None

# ESç›¸å…³
from elasticsearch import Elasticsearch
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ", version="2.0.0")

# å¯¼å…¥é…ç½®
from config import (
    ES_CONFIG, DOCUMENT_CONFIG, EMBEDDING_CONFIG, RAG_CONFIG,
    PYMUPDF_PRO_CONFIG, CHUNKING_CONFIG, GEEKAI_API_KEY, GEEKAI_CHAT_URL,
    GEEKAI_EMBEDDING_URL, DEFAULT_EMBEDDING_MODEL
)

# è¾…åŠ©ï¼šæ„å»ºé¡µæ–‡æœ¬ä¸è¯çº§ç´¢å¼•ï¼ˆç”¨äºbboxå®šä½ï¼‰
def build_page_text_and_word_index(page: "pymupdf.Page") -> (str, list):
    """è¿”å› (page_text, word_entries), å…¶ä¸­ word_entries ä¸º [ (start, end, (x0,y0,x1,y1)) ]."""
    try:
        words = page.get_text("words")  # (x0,y0,x1,y1,word,block_no,line_no,word_no)
        # æ’åºï¼šå—â†’è¡Œâ†’è¯
        words_sorted = sorted(words, key=lambda w: (int(w[5]), int(w[6]), int(w[7])))
        parts = []
        entries = []
        last_block, last_line = None, None
        current_pos = 0
        for w in words_sorted:
            x0, y0, x1, y1, token, bno, lno, wno = w
            key = (bno, lno)
            if last_block is None:
                # é¦–è¯ï¼Œç›´æ¥å†™
                parts.append(token)
                start = current_pos
                end = start + len(token)
                entries.append((start, end, (float(x0), float(y0), float(x1), float(y1))))
                current_pos = end
            else:
                if key != (last_block, last_line):
                    # æ¢è¡Œ
                    parts.append("\n")
                    current_pos += 1
                    parts.append(token)
                    start = current_pos
                    end = start + len(token)
                    entries.append((start, end, (float(x0), float(y0), float(x1), float(y1))))
                    current_pos = end
                else:
                    # åŒè¡Œè¯ï¼Œç©ºæ ¼åˆ†éš”
                    parts.append(" ")
                    current_pos += 1
                    parts.append(token)
                    start = current_pos
                    end = start + len(token)
                    entries.append((start, end, (float(x0), float(y0), float(x1), float(y1))))
                    current_pos = end
            last_block, last_line = key
        page_text = "".join(parts)
        return page_text, entries
    except Exception:
        txt = page.get_text() or ""
        return txt, []

def compute_bbox_union_for_range(entries: list, start: int, end: int) -> list:
    """æ ¹æ®è¯çº§ç´¢å¼•åˆ—è¡¨ï¼Œè®¡ç®—è¦†ç›– [start,end) çš„bboxå¹¶é›†ï¼Œè¿”å› [x0,y0,x1,y1]ï¼Œæ— åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚"""
    x0 = y0 = float("inf")
    x1 = y1 = float("-inf")
    found = False
    for (s, e, (a, b, c, d)) in entries:
        if e <= start or s >= end:
            continue
        found = True
        x0 = min(x0, a)
        y0 = min(y0, b)
        x1 = max(x1, c)
        y1 = max(y1, d)
    return [x0, y0, x1, y1] if found else []

# åˆå§‹åŒ–ESå®¢æˆ·ç«¯
es_client = Elasticsearch(
    [f"http://{ES_CONFIG['host']}:{ES_CONFIG['port']}"],
    basic_auth=(ES_CONFIG['username'], ES_CONFIG['password']) if ES_CONFIG['username'] else None,
    verify_certs=ES_CONFIG['verify_certs']
)

# æå®¢API embeddingå‡½æ•°
def get_embedding(text: str) -> list:
    """ä½¿ç”¨æå®¢APIè·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
    try:
        url = GEEKAI_EMBEDDING_URL
        headers = {
            "Authorization": f"Bearer {GEEKAI_API_KEY}",
            "Content-Type": "application/json"
        }
        data = {
            "model": DEFAULT_EMBEDDING_MODEL,
            "input": [text],
            "intent": "search_document"
        }
        response = requests.post(url, headers=headers, json=data, timeout=20)
        if response.status_code == 200:
            return response.json().get("data", [{}])[0].get("embedding")
        else:
            logger.error(f"æå®¢API embeddingå¤±è´¥: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        logger.error(f"æå®¢API embeddingå¼‚å¸¸: {e}")
        return None

# è¯·æ±‚æ¨¡å‹
class LdapValidateRequest(BaseModel):
    username: str
    password: str

class ChatRequest(BaseModel):
    question: str
    user_id: str

class DocumentProcessRequest(BaseModel):
    knowledge_id: int
    knowledge_name: str
    description: str
    tags: List[str]
    effective_time: str

# å“åº”æ¨¡å‹
class LdapValidateResponse(BaseModel):
    success: bool
    message: str
    email: Optional[str] = None
    role: Optional[str] = None

class KnowledgeReference(BaseModel):
    knowledge_id: int
    knowledge_name: str
    description: str
    tags: List[str]
    effective_time: str
    attachments: List[str]
    relevance: float
    # è¿½æº¯å®šä½å¢å¼º
    source_file: Optional[str] = None
    page_num: Optional[int] = None
    chunk_index: Optional[int] = None
    chunk_type: Optional[str] = None
    # æ–°å¢ï¼šè¿”å›å—åæ ‡ä¸å­—ç¬¦èŒƒå›´ï¼Œä¾¿äºå‰ç«¯é«˜äº®
    bbox_union: Optional[List[float]] = None
    char_start: Optional[int] = None
    char_end: Optional[int] = None

class ChatResponse(BaseModel):
    answer: str
    references: List[KnowledgeReference]
    session_id: Optional[str] = None

class DocumentProcessResponse(BaseModel):
    success: bool
    message: str
    chunks_count: int
    knowledge_id: int

@app.get("/")
def read_root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ",
        "version": "2.0.0",
        "features": [
            "PyMuPDF Pro æ–‡æ¡£å¤„ç†",
            "PyMuPDF4LLM ç»“æ„åŒ–åˆ†å—", 
            "LangChain å‘é‡åŒ–",
            "Elasticsearch å­˜å‚¨",
            "æå®¢æ™ºåŠAPI æ™ºèƒ½é—®ç­”"
        ]
    }

@app.post("/api/ldap/validate", response_model=LdapValidateResponse)
def validate_ldap_user(request: LdapValidateRequest):
    """LDAPç”¨æˆ·éªŒè¯ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    logger.info(f"LDAPéªŒè¯è¯·æ±‚: {request.username}")
    
    # æ¨¡æ‹ŸLDAPéªŒè¯
    if request.username == "admin" and request.password == "password":
        return LdapValidateResponse(
            success=True,
            message="éªŒè¯æˆåŠŸ",
            email="admin@example.com",
            role="admin"
        )
    else:
        return LdapValidateResponse(
            success=False,
            message="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"
        )

def process_document_unified(file_path: str, knowledge_id: int, knowledge_name: str, 
                           description: str, tags: str, effective_time: str, original_filename: str = None):
    """
    ä½¿ç”¨ PyMuPDF ç»Ÿä¸€å¤„ç†æ–‡æ¡£ï¼Œç”ŸæˆPDFLLMé£æ ¼çš„è¾“å‡º
    """
    logger.info(f"å¼€å§‹ç»Ÿä¸€å¤„ç†æ–‡æ¡£: {file_path}")
    
    try:
        # ä½¿ç”¨ PyMuPDF æ‰“å¼€æ–‡æ¡£
        doc = pymupdf.open(file_path)
        logger.info(f"æˆåŠŸæ‰“å¼€æ–‡æ¡£ï¼Œé¡µæ•°: {len(doc)}")

        # å¯¹éPDFæ–‡æ¡£ï¼Œå…ˆè½¬æ¢ä¸ºæ ‡å‡†PDF
        input_suffix = Path(file_path).suffix.lower()
        use_pdf_doc = doc
        try:
            if input_suffix != ".pdf":
                logger.info("æ£€æµ‹åˆ°éPDFæ–‡æ¡£ï¼Œå¼€å§‹è½¬æ¢ä¸ºPDFâ€¦")
                pdf_bytes = doc.convert_to_pdf()
                use_pdf_doc = pymupdf.open(stream=pdf_bytes, filetype="pdf")
                logger.info(f"è½¬æ¢å®Œæˆï¼ŒPDFé¡µæ•°: {len(use_pdf_doc)}")
        except Exception as e:
            logger.warning(f"è½¬æ¢PDFå¤±è´¥ï¼Œå›é€€ä½¿ç”¨åŸæ–‡æ¡£æå–ï¼š{e}")
        
        # ä½¿ç”¨PyMuPDFç”Ÿæˆå¹²å‡€çš„Markdownå†…å®¹
        md_text = generate_pdfllm_style_markdown(use_pdf_doc, original_filename or Path(file_path).name)
        logger.info(f"ç”Ÿæˆäº†å¹²å‡€çš„Markdownå†…å®¹ï¼Œé•¿åº¦: {len(md_text)}")
        
        # å•ç‹¬æå–ä½ç½®ä¿¡æ¯æ˜ å°„
        position_mapping = extract_position_mapping(use_pdf_doc)
        logger.info(f"æå–å‡º {len(position_mapping)} ä¸ªä½ç½®ä¿¡æ¯é¡¹")
        
        if not position_mapping:
            logger.warning("æ²¡æœ‰ä½ç½®ä¿¡æ¯é¡¹ï¼Œä¸ºæ‰€æœ‰chunksè®¾ç½®é»˜è®¤å…ƒæ•°æ®")
        
        # ä½¿ç”¨LangChainè¿›è¡Œåˆ†å— - ä¿æŒé€‚ä¸­çš„åˆ†å—å¤§å°
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,  # é€‚ä¸­çš„åˆ†å—å¤§å°
            chunk_overlap=300,  # é€‚ä¸­çš„é‡å 
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        doc_obj = Document(page_content=md_text, metadata={"source": file_path})
        chunks = text_splitter.split_documents([doc_obj])
        logger.info(f"åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªchunks")
        
        # ä¸ºæ¯ä¸ªchunkåˆ†é…ä½ç½®ä¿¡æ¯å’Œå¢å¼ºå…ƒæ•°æ®
        for i, chunk in enumerate(chunks):
            # åŸºç¡€å…ƒæ•°æ®
            chunk.metadata.update({
                "knowledge_id": knowledge_id,
                "knowledge_name": knowledge_name,
                "description": description,
                "tags": tags,
                "effective_time": effective_time,
                "source_file": original_filename or Path(file_path).name,
                "chunk_index": i,
                "chunk_type": "content"
            })
            
            # å¢å¼ºå…ƒæ•°æ® - åªä¿ç•™é€šç”¨ä¿¡æ¯
            chunk.metadata.update({
                "document_name": original_filename or Path(file_path).name,  # æ–‡æ¡£åç§°
                "document_type": "æ–‡æ¡£",  # æ–‡æ¡£ç±»å‹ï¼ˆé€šç”¨ï¼‰
                "keywords": extract_keywords_from_content(chunk.page_content),  # å…³é”®æ ‡è¯†è¯
            })
            
            # å°è¯•ä¸ºchunkåˆ†é…ä½ç½®ä¿¡æ¯
            if position_mapping and len(position_mapping) > 0:
                # ç®€åŒ–ä½ç½®åŒ¹é…é€»è¾‘ - è®©å¤§æ¨¡å‹è‡ªå·±åˆ¤æ–­
                chunk_text = chunk.page_content
                best_match = find_best_position_match(chunk_text, position_mapping)
                
                if best_match:
                    # è®¡ç®—å­—ç¬¦èŒƒå›´
                    char_start = md_text.find(chunk_text)
                    char_end = char_start + len(chunk_text) if char_start != -1 else -1
                    
                    chunk.metadata.update({
                        "page_num": best_match.get("page", 1),
                        "bbox_union": best_match.get("bbox", []),
                        "char_start": char_start,
                        "char_end": char_end
                    })
                    logger.info(f"Chunk {i} åˆ†é…ä½ç½®ä¿¡æ¯: é¡µ{best_match.get('page', 1)}, bbox={best_match.get('bbox', [])}, chars=({char_start}, {char_end})")
                else:
                    logger.warning(f"Chunk {i} æœªæ‰¾åˆ°åŒ¹é…çš„ä½ç½®ä¿¡æ¯")
                    # è®¾ç½®é»˜è®¤ä½ç½®ä¿¡æ¯
                    chunk.metadata.update({
                        "page_num": 1,
                        "bbox_union": [],
                        "char_start": -1,
                        "char_end": -1
                    })
            else:
                # æ²¡æœ‰ä½ç½®ä¿¡æ¯ï¼Œè®¾ç½®é»˜è®¤å€¼
                chunk.metadata.update({
                    "page_num": 1,
                    "bbox_union": [],
                    "char_start": -1,
                    "char_end": -1
                })
        
        # ç”Ÿæˆembeddingså¹¶å­˜å‚¨åˆ°ES
        chunks_with_embeddings = []
        for chunk in chunks:
            try:
                embedding = get_embedding(chunk.page_content)
                chunk.metadata["embedding"] = embedding
                chunks_with_embeddings.append(chunk)
            except Exception as e:
                logger.error(f"ç”Ÿæˆembeddingå¤±è´¥: {e}")
                continue
        
        # å­˜å‚¨åˆ°ES
        if chunks_with_embeddings:
            store_chunks_to_es(chunks_with_embeddings, knowledge_id)
            logger.info(f"æˆåŠŸå­˜å‚¨ {len(chunks_with_embeddings)} ä¸ªchunksåˆ°ES")
        else:
            logger.error("æ²¡æœ‰å¯å­˜å‚¨çš„chunks")
            raise Exception("æ–‡æ¡£å¤„ç†å¤±è´¥ï¼šæ²¡æœ‰å¯å­˜å‚¨çš„chunks")
        
        # æ¸…ç†èµ„æº
        doc.close()
        if use_pdf_doc != doc:
            use_pdf_doc.close()
        
        return {
            "success": True,
            "chunks_count": len(chunks_with_embeddings),
            "message": f"æ–‡æ¡£å¤„ç†æˆåŠŸ: {original_filename or Path(file_path).name}"
        }
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        raise

def generate_pdfllm_style_markdown(doc, filename: str) -> str:
    """
    ä½¿ç”¨PyMuPDFç”Ÿæˆå¹²å‡€çš„Markdownå†…å®¹ï¼Œä½ç½®ä¿¡æ¯ä¸åµŒå…¥åˆ°æ–‡æœ¬ä¸­
    """
    content_lines = []
    
    # æ·»åŠ æ–‡æ¡£æ ‡é¢˜
    content_lines.append(f"# {filename}")
    content_lines.append("")
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        
        # é¡µé¢æ ‡é¢˜
        content_lines.append(f"## ç¬¬ {page_num + 1} é¡µ")
        content_lines.append("")
        
        # è·å–é¡µé¢æ–‡æœ¬å—
        blocks = page.get_text("dict")
        
        if "blocks" in blocks:
            for block_idx, block in enumerate(blocks["blocks"]):
                if "lines" in block:
                    block_text = ""
                    
                    for line_idx, line in enumerate(block["lines"]):
                        line_text = ""
                        
                        for span_idx, span in enumerate(line["spans"]):
                            text = span["text"].strip()
                            if text:
                                # åªæ·»åŠ æ–‡æœ¬å†…å®¹ï¼Œä¸æ·»åŠ ä½ç½®æ ‡ç­¾
                                line_text += text + " "
                        
                        if line_text.strip():
                            block_text += line_text.strip() + "\n"
                    
                    if block_text.strip():
                        # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯æ ‡é¢˜ï¼ˆåŸºäºå­—ä½“å¤§å°ï¼‰
                        max_font_size = max(span["size"] for line in block["lines"] for span in line["spans"])
                        if max_font_size > 12:  # å‡è®¾å¤§äº12ptçš„æ˜¯æ ‡é¢˜
                            content_lines.append(f"### {block_text.strip()}")
                        else:
                            content_lines.append(block_text.strip())
                        content_lines.append("")
    
    # åˆå¹¶æ‰€æœ‰å†…å®¹
    return "\n".join(content_lines)

def extract_position_mapping(doc) -> List[Dict]:
    """
    å•ç‹¬æå–ä½ç½®ä¿¡æ¯æ˜ å°„ï¼Œä¸åµŒå…¥åˆ°æ–‡æœ¬å†…å®¹ä¸­
    """
    position_mapping = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        blocks = page.get_text("dict")
        
        if "blocks" in blocks:
            for block_idx, block in enumerate(blocks["blocks"]):
                if "lines" in block:
                    for line_idx, line in enumerate(block["lines"]):
                        for span_idx, span in enumerate(line["spans"]):
                            text = span["text"].strip()
                            if text:
                                position_mapping.append({
                                    "text": text,
                                    "page": page_num + 1,
                                    "bbox": span["bbox"],
                                    "font_size": span["size"],
                                    "font": span["font"],
                                    "block_idx": block_idx,
                                    "line_idx": line_idx,
                                    "span_idx": span_idx
                                })
    
    return position_mapping

def parse_pdfllm_style_markdown(md_text: str) -> List[Dict]:
    """
    è§£æPDFLLMé£æ ¼çš„Markdownï¼Œæå–ä½ç½®ä¿¡æ¯
    """
    items = []
    
    # åŒ¹é… <sub>pos: page=X, bbox=(...)</sub> æ ¼å¼
    import re
    pattern = r'<sub>pos: page=(\d+), bbox=\(([^)]+)\)</sub>'
    
    matches = re.findall(pattern, md_text)
    for match in matches:
        page_num = int(match[0])
        bbox_str = match[1]
        
        try:
            # è§£æbboxå­—ç¬¦ä¸² "x0, y0, x1, y1"
            bbox_parts = bbox_str.split(',')
            if len(bbox_parts) == 4:
                bbox = [float(part.strip()) for part in bbox_parts]
                
                # æå–å¯¹åº”çš„æ–‡æœ¬å†…å®¹
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
                items.append({
                    "page": page_num,
                    "bbox": bbox,
                    "char_start": -1,  # ç®€åŒ–å¤„ç†
                    "char_end": -1,    # ç®€åŒ–å¤„ç†
                    "text": ""          # ç®€åŒ–å¤„ç†
                })
        except Exception as e:
            logger.warning(f"è§£æbboxå¤±è´¥: {bbox_str}, é”™è¯¯: {e}")
            continue
                
    return items

def find_best_position_match(chunk_text: str, position_mapping: List[Dict]) -> Optional[Dict]:
    """
    ä¸ºchunkæ‰¾åˆ°æœ€åŒ¹é…çš„ä½ç½®ä¿¡æ¯
    """
    if not position_mapping:
        return None
    
    # æ”¹è¿›çš„åŒ¹é…é€»è¾‘ï¼šä¼˜å…ˆåŒ¹é…åŒ…å«åœ¨chunkä¸­çš„æ–‡æœ¬
    chunk_text_lower = chunk_text.lower()
    chunk_words = set(chunk_text_lower.split())
    
    best_match = None
    best_score = 0
    
    # ç¬¬ä¸€è½®ï¼šå¯»æ‰¾ç²¾ç¡®åŒ…å«çš„æ–‡æœ¬
    for pos_info in position_mapping:
        text = pos_info.get("text", "").strip()
        if text and text.lower() in chunk_text_lower:
            # è®¡ç®—åŒ¹é…åº¦ï¼šæ–‡æœ¬é•¿åº¦ä¸chunké•¿åº¦çš„æ¯”ä¾‹
            score = len(text) / max(len(chunk_text), 1)
            if score > best_score:
                best_score = score
                best_match = pos_info
    
    # ç¬¬äºŒè½®ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ…å«çš„ï¼Œå°è¯•å•è¯åŒ¹é…
    if not best_match:
        for pos_info in position_mapping:
            text = pos_info.get("text", "").strip()
            if text:
                text_words = set(text.lower().split())
                # è®¡ç®—å•è¯é‡å åº¦
                overlap = len(chunk_words.intersection(text_words))
                if overlap > 0:
                    score = overlap / max(len(chunk_words), 1)
                    if score > best_score:
                        best_score = score
                        best_match = pos_info
    
    # ç¬¬ä¸‰è½®ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ä½ç½®ä¿¡æ¯
    if not best_match and position_mapping:
        for pos_info in position_mapping:
            if pos_info.get("bbox") and len(pos_info.get("bbox", [])) == 4:
                return pos_info
    
    return best_match

def store_chunks_to_es(chunks: List[Document], knowledge_id: int):
    """
    å°†chunkså­˜å‚¨åˆ°Elasticsearch
    """
    stored_count = 0
    
    for i, chunk in enumerate(chunks):
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = hashlib.md5(f"{knowledge_id}_{i}_{chunk.page_content[:100]}".encode()).hexdigest()
            
            # å‡†å¤‡ESæ–‡æ¡£
            es_doc = {
                "content": chunk.page_content,
                "embedding": chunk.metadata.get("embedding", []),
                "knowledge_id": chunk.metadata.get("knowledge_id", knowledge_id),
                "knowledge_name": chunk.metadata.get("knowledge_name", ""),
                "description": chunk.metadata.get("description", ""),
                "tags": chunk.metadata.get("tags", ""),
                "effective_time": chunk.metadata.get("effective_time", ""),
                "source_file": chunk.metadata.get("source_file", ""),
                "chunk_index": chunk.metadata.get("chunk_index", i),
                "chunk_type": chunk.metadata.get("chunk_type", "content"),
                "page_num": chunk.metadata.get("page_num", 1),
                "char_start": chunk.metadata.get("char_start", -1),
                "char_end": chunk.metadata.get("char_end", -1),
                "bbox_union": chunk.metadata.get("bbox_union", []),
                "weight": 1.0
            }
            
            # å­˜å‚¨åˆ°ES
            es_client.index(index=ES_CONFIG['index'], id=doc_id, document=es_doc)
            stored_count += 1
            
            if (i + 1) % 10 == 0:
                logger.info(f"å·²å­˜å‚¨ {i + 1}/{len(chunks)} ä¸ªchunks")
                
        except Exception as e:
            logger.error(f"å­˜å‚¨chunk {i} å¤±è´¥: {e}")
            continue

    logger.info(f"ESå­˜å‚¨å®Œæˆï¼ŒæˆåŠŸå­˜å‚¨ {stored_count}/{len(chunks)} ä¸ªchunks")
    return stored_count

@app.post("/api/document/process", response_model=DocumentProcessResponse)
async def process_document(
    file: UploadFile = File(...),
    knowledge_id: int = Form(None),
    knowledge_name: str = Form(None),
    description: str = Form(None),
    tags: str = Form(None),
    effective_time: str = Form(None)
):
    """
    ä½¿ç”¨ PyMuPDF Pro + PyMuPDF4LLM ç»Ÿä¸€å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£
    æ”¯æŒ PDFã€Wordã€Excelã€PowerPointã€TXT ç­‰æ ¼å¼
    """
    logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file.filename}, çŸ¥è¯†ID: {knowledge_id}")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        file_extension = Path(file.filename).suffix.lower()
        
        # PyMuPDF Pro æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {
            ".pdf", ".docx", ".doc", ".xlsx", ".xls", 
            ".pptx", ".ppt", ".txt", ".hwp", ".hwpx"
        }
        
        if file_extension not in supported_extensions:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}")
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # å¤„ç†æ–‡æ¡£
            result = process_document_unified(
                temp_file_path, knowledge_id, knowledge_name, 
                description, tags, effective_time, file.filename
            )
            
            return DocumentProcessResponse(
                success=True,
                message=f"æ–‡æ¡£å¤„ç†æˆåŠŸ: {file.filename}",
                chunks_count=result["chunks_count"],
                knowledge_id=int(knowledge_id) if knowledge_id is not None else 0
            )
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file_path)
            
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

@app.post("/api/rag/chat", response_model=ChatResponse)
def chat_with_rag(request: ChatRequest):
    """
    åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”
    """
    logger.info(f"RAGèŠå¤©è¯·æ±‚: {request.question}")
    
    try:
        # 1. å‘é‡æœç´¢æ‰¾åˆ°ç›¸å…³chunks
        question_embedding = get_embedding(request.question)
        if not question_embedding:
            return ChatResponse(
                answer="æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆé—®é¢˜çš„å‘é‡è¡¨ç¤ºï¼Œè¯·é‡è¯•ã€‚",
                references=[],
                session_id=request.user_id
            )
        
        # ç®€åŒ–æœç´¢é€»è¾‘ - ç›´æ¥è¿”å›è¯­ä¹‰ç›¸ä¼¼åº¦æœ€é«˜çš„chunks
        search_query = {
            "size": 5,  # è¿”å›å‰5ä¸ªæœ€ç›¸å…³çš„chunks
            "query": {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": question_embedding}
                    }
                }
            },
            "_source": ["content", "metadata", "knowledge_id", "knowledge_name", "source_file", 
                       "page_num", "chunk_index", "bbox_union", "char_start", "char_end"]
        }
        
        search_response = es_client.search(index=ES_CONFIG['index'], body=search_query)
        hits = search_response.get('hits', {}).get('hits', [])
        
        if not hits:
            return ChatResponse(
                answer="æŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                references=[],
                session_id=request.user_id
            )
        
        # 2. æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_chunks = []
        for hit in hits:
            source = hit['_source']
            score = hit['_score']
            
            # æ„å»ºæ¯ä¸ªchunkçš„å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯
            chunk_info = {
                "content": source.get('content', ''),
                "metadata": {
                    "document_name": source.get('source_file', 'N/A'),
                    "document_type": "æ–‡æ¡£",  # é€šç”¨æ–‡æ¡£ç±»å‹
                    "page_num": source.get('page_num', 'N/A'),
                    "chunk_index": source.get('chunk_index', 'N/A'),
                    "bbox_union": source.get('bbox_union', []),
                    "char_start": source.get('char_start', 'N/A'),
                    "char_end": source.get('char_end', 'N/A'),
                    "knowledge_name": source.get('knowledge_name', 'N/A'),
                    "relevance_score": round(score, 3)
                }
            }
            context_chunks.append(chunk_info)
        
        # 3. æ„å»ºå¢å¼ºçš„RAGæç¤ºè¯
        enhanced_prompt = build_enhanced_rag_prompt(request.question, context_chunks)
        
        # 4. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
        answer = generate_ai_answer(enhanced_prompt)
        
        # 5. æ„å»ºå¼•ç”¨ä¿¡æ¯
        references = []
        for chunk in context_chunks:
            metadata = chunk['metadata']
            references.append(KnowledgeReference(
                knowledge_id=0,  # è¿™é‡Œéœ€è¦ä»chunkä¸­è·å–
                knowledge_name=metadata.get('knowledge_name', ''),
                description=f"æ–‡æ¡£: {metadata.get('document_name', '')}",
                tags=[metadata.get('document_type', '')],
                effective_time="",
                attachments=[metadata.get('document_name', '')],
                relevance=metadata.get('relevance_score', 0.0),
                source_file=metadata.get('document_name', ''),
                page_num=metadata.get('page_num', 0),
                chunk_index=metadata.get('chunk_index', 0),
                chunk_type="content",
                bbox_union=metadata.get('bbox_union', []),
                char_start=metadata.get('char_start', 0),
                char_end=metadata.get('char_end', 0)
            ))
        
        return ChatResponse(
            answer=answer,
            references=references,
            session_id=request.user_id
        )
        
    except Exception as e:
        logger.error(f"RAGèŠå¤©å¤±è´¥: {e}")
        return ChatResponse(
            answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
            references=[],
            session_id=request.user_id
        )

def build_enhanced_rag_prompt(question: str, context_chunks: List[Dict]) -> str:
    """
    æ„å»ºå¢å¼ºçš„RAGæç¤ºè¯ï¼Œè®©å¤§æ¨¡å‹èƒ½çœ‹åˆ°å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    context_parts = []
    
    for i, chunk in enumerate(context_chunks):
        metadata = chunk['metadata']
        
        # æ„å»ºæ¯ä¸ªchunkçš„è¯¦ç»†ä¸Šä¸‹æ–‡ä¿¡æ¯
        chunk_context = f"""
=== å¼•ç”¨ {i+1} ===
ğŸ“„ æ–‡æ¡£åç§°: {metadata.get('document_name', 'N/A')}
ğŸ“‹ æ–‡æ¡£ç±»å‹: {metadata.get('document_type', 'N/A')}
ğŸ“– é¡µç : {metadata.get('page_num', 'N/A')}
ğŸ”¢ å—åº: {metadata.get('chunk_index', 'N/A')}
ğŸ¯ ç›¸å…³æ€§: {metadata.get('relevance_score', 'N/A')}
ğŸ“ åæ ‡: {metadata.get('bbox_union', [])}
ğŸ“ å†…å®¹: {chunk.get('content', '')}
"""
        context_parts.append(chunk_context)
    
    # æ„å»ºå®Œæ•´æç¤º
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

æ¯ä¸ªå¼•ç”¨éƒ½åŒ…å«äº†å®Œæ•´çš„æ–‡æ¡£ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æ¡£åç§°å’Œç±»å‹
- é¡µç å’Œå—åº
- ç›¸å…³æ€§è¯„åˆ†å’Œä½ç½®åæ ‡
- å…·ä½“å†…å®¹

è¯·ä»”ç»†åˆ†æè¿™äº›ä¿¡æ¯ï¼Œå¹¶æ ¹æ®é—®é¢˜æ‰¾åˆ°æœ€å‡†ç¡®çš„ç­”æ¡ˆã€‚

é—®é¢˜: {question}

å‚è€ƒä¿¡æ¯:
{''.join(context_parts)}

è¯·æ ¹æ®é—®é¢˜ï¼Œä»ä¸Šè¿°ä¿¡æ¯ä¸­æ‰¾åˆ°æœ€å‡†ç¡®çš„ç­”æ¡ˆã€‚è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦è¯´"è¯·æŸ¥çœ‹å¼•ç”¨ä¿¡æ¯"
2. å¦‚æœé—®é¢˜æ¶‰åŠç‰¹å®šæ–‡æ¡£ï¼Œè¯·ç¡®ä¿ç­”æ¡ˆæ¥è‡ªæ­£ç¡®çš„æ–‡æ¡£
3. å¦‚æœé—®é¢˜æ²¡æœ‰æŒ‡å®šå…·ä½“æ–‡æ¡£ï¼Œè¯·åŸºäºæ‰€æœ‰ç›¸å…³ä¿¡æ¯ç»™å‡ºç»¼åˆå›ç­”
4. ç­”æ¡ˆè¦å…·ä½“ã€å‡†ç¡®ï¼ŒåŒ…å«å…³é”®æ•°æ®
5. ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶è¯´æ˜ä¿¡æ¯æ¥æºï¼ˆå¦‚"æ ¹æ®æ–‡æ¡£ç¬¬Xé¡µ"ï¼‰

è¯·å¼€å§‹å›ç­”ï¼š
"""
    
    return prompt

def generate_ai_answer(prompt: str) -> str:
    """
    è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
    """
    try:
        # è°ƒç”¨æå®¢æ™ºåŠAPIç”Ÿæˆç­”æ¡ˆ
        headers = {
            "Authorization": f"Bearer {GEEKAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-4o-mini",  # ä½¿ç”¨åˆé€‚çš„æ¨¡å‹
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æ–‡æ¡£ä¿¡æ¯å‡†ç¡®å›ç­”é—®é¢˜ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.3
        }
        
        response = requests.post(
            GEEKAI_CHAT_URL,
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                answer = result["choices"][0]["message"]["content"]
                return answer
            else:
                logger.error(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°æ ¼å¼é”™è¯¯ï¼Œè¯·é‡è¯•ã€‚"
        else:
            logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}, {response.text}")
            return f"æŠ±æ­‰ï¼ŒAPIè°ƒç”¨å¤±è´¥ï¼ˆçŠ¶æ€ç ï¼š{response.status_code}ï¼‰ï¼Œè¯·é‡è¯•ã€‚"
            
    except requests.exceptions.Timeout:
        logger.error("APIè°ƒç”¨è¶…æ—¶")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆè¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚"
    except requests.exceptions.RequestException as e:
        logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
        return f"æŠ±æ­‰ï¼ŒAPIè¯·æ±‚å¼‚å¸¸ï¼š{str(e)}ï¼Œè¯·é‡è¯•ã€‚"
    except Exception as e:
        logger.error(f"ç”ŸæˆAIç­”æ¡ˆå¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}ï¼Œè¯·é‡è¯•ã€‚"

def extract_keywords_from_content(content: str) -> List[str]:
    """ä»å†…å®¹æå–å…³é”®æ ‡è¯†è¯ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰"""
    keywords = []
    content_lower = content.lower()
    
    # é€šç”¨å…³é”®è¯
    general_keywords = ["æŠ•èµ„", "ç®¡ç†", "é£é™©", "æ”¶ç›Š", "è´¹ç”¨", "æ—¥æœŸ", "ç›®æ ‡", "ç­–ç•¥", "æŠ¥å‘Š", "åˆ†æ"]
    for keyword in general_keywords:
        if keyword in content_lower:
            keywords.append(keyword)
    
    # æ–‡æ¡£ç»“æ„å…³é”®è¯
    structure_keywords = ["æ ‡é¢˜", "ç« èŠ‚", "è¡¨æ ¼", "å›¾è¡¨", "é™„å½•", "æ‘˜è¦", "ç»“è®º"]
    for keyword in structure_keywords:
        if keyword in content_lower:
            keywords.append(keyword)
    
    return keywords[:8]  # é™åˆ¶å…³é”®è¯æ•°é‡

@app.get("/api/health")
def health_check():
    """
    å¥åº·æ£€æŸ¥
    """
    try:
        # æ£€æŸ¥ESè¿æ¥
        es_info = es_client.info()
        
        # æ£€æŸ¥PyMuPDF / Pro å¯ç”¨æ€§
        pymupdf_status = "available" if PYMUPDF_AVAILABLE else "unavailable"
        pymupdf_pro_status = "available" if PYMUPDF_PRO_AVAILABLE else "unavailable"
        
        # æ£€æŸ¥æå®¢æ™ºåŠAPI
        try:
            headers = {"Authorization": f"Bearer {GEEKAI_API_KEY}"}
            response = requests.get("https://geekai.co/api/v1/models", headers=headers, timeout=5)
            geekai_status = "available" if response.status_code == 200 else "unavailable"
        except:
            geekai_status = "unavailable"
        
        return {
            "status": "healthy",
            "elasticsearch": "connected",
            "pymupdf": pymupdf_status,
            "pymupdf_pro": pymupdf_pro_status,
            "geekai_api": geekai_status,
            "mypymupdf4llm": "available" if PYMUPDF4LLM_AVAILABLE else "unavailable",
            "timestamp": "2024-01-01T00:00:00Z"
        }
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
