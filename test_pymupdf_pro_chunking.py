#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyMuPDF Pro æ–‡ä»¶åˆ‡å—æµ‹è¯•è„šæœ¬
æµ‹è¯•PyMuPDF Proå¯¹ä¸åŒæ–‡ä»¶ç±»å‹çš„åˆ‡å—æ•ˆæœ
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ python_serviceç›®å½•åˆ°è·¯å¾„
sys.path.append('python_service')

# å¯¼å…¥PyMuPDF Proç›¸å…³
from python_service.pymupdf_font_fix import setup_pymupdf_pro_environment, test_pymupdf_pro_initialization

# åˆå§‹åŒ– PyMuPDF Pro ç¯å¢ƒ
if not setup_pymupdf_pro_environment():
    print("âŒ PyMuPDF Pro ç¯å¢ƒè®¾ç½®å¤±è´¥")
    sys.exit(1)

# æµ‹è¯• PyMuPDF Pro åˆå§‹åŒ–
if not test_pymupdf_pro_initialization():
    print("âŒ PyMuPDF Pro åˆå§‹åŒ–å¤±è´¥")
    sys.exit(1)

import pymupdf.pro

# æ–‡æ¡£å¤„ç†ç›¸å…³
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain.schema import Document

# PyMuPDF4LLM ç”¨äºç»“æ„åŒ–åˆ†å—
try:
    from pymupdf4llm import LlamaMarkdownReader
    PYMUPDF4LLM_AVAILABLE = True
except ImportError:
    print("âš ï¸ PyMuPDF4LLM ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿåˆ†å—")
    PYMUPDF4LLM_AVAILABLE = False

# å¯¼å…¥é…ç½®
from python_service.config import DOCUMENT_CONFIG, CHUNKING_CONFIG

class PyMuPDFProChunkingTester:
    def __init__(self):
        self.results = {}
        self.test_files = {
            'pdf': ['python_service/file/å®‰è”ç¾å…ƒ.pdf', 'python_service/file/åº—é“ºå…¥ä½æµç¨‹.pdf'],
            'docx': ['python_service/file/manusä»‹ç».docx'],
            'xlsx': ['python_service/file/å°çº¢ä¹¦é€‰å“.xlsx'],
            'pptx': ['python_service/file/network_skill.pptx']
        }
        
    def analyze_coherence(self, chunks: List[Document]) -> Dict[str, Any]:
        """åˆ†æåˆ†å—çš„è¿è´¯æ€§"""
        coherence_analysis = {
            'total_chunks': len(chunks),
            'coherence_score': 0,
            'issues': [],
            'strengths': []
        }
        
        if len(chunks) < 2:
            return coherence_analysis
        
        # åˆ†æè¿è´¯æ€§é—®é¢˜
        issues = []
        strengths = []
        
        for i, chunk in enumerate(chunks):
            content = chunk.page_content
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å éƒ¨åˆ†ï¼ˆè¿™æ˜¯ä¼˜ç‚¹ï¼‰
            if i > 0:
                prev_content = chunks[i-1].page_content
                overlap = self.find_overlap(prev_content, content)
                if overlap > 20:  # é‡å è¶…è¿‡20å­—ç¬¦
                    strengths.append(f"å—{i}å’Œ{i+1}: æœ‰{overlap}å­—ç¬¦é‡å ï¼Œä¿æŒè¿è´¯æ€§")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æ®µè½ä¸­é—´åˆ‡æ–­ï¼ˆåªåœ¨æ²¡æœ‰é‡å æ—¶æ‰ç®—é—®é¢˜ï¼‰
            if i > 0:
                prev_content = chunks[i-1].page_content
                overlap = self.find_overlap(prev_content, content)
                if overlap < 10 and content.count('\n\n') < 1 and len(content) > 300:
                    issues.append(f"å—{i+1}: å¯èƒ½åœ¨æ®µè½ä¸­é—´åˆ‡æ–­ä¸”é‡å ä¸è¶³")
            
            # æ£€æŸ¥å—é•¿åº¦æ˜¯å¦åˆç†
            if len(content) < 100:
                issues.append(f"å—{i+1}: é•¿åº¦è¿‡çŸ­({len(content)}å­—ç¬¦)")
            elif len(content) > 2000:
                issues.append(f"å—{i+1}: é•¿åº¦è¿‡é•¿({len(content)}å­—ç¬¦)")
        
        # è®¡ç®—è¿è´¯æ€§åˆ†æ•°
        total_issues = len(issues)
        total_strengths = len(strengths)
        # è°ƒæ•´è¯„åˆ†é€»è¾‘ï¼šé‡å æ˜¯ä¼˜ç‚¹ï¼Œå‡å°‘é—®é¢˜æƒé‡
        coherence_score = max(0, 100 - total_issues * 5 + total_strengths * 10)
        
        coherence_analysis.update({
            'coherence_score': coherence_score,
            'issues': issues,
            'strengths': strengths
        })
        
        return coherence_analysis
    
    def find_overlap(self, text1: str, text2: str) -> int:
        """æŸ¥æ‰¾ä¸¤ä¸ªæ–‡æœ¬çš„é‡å éƒ¨åˆ†"""
        words1 = text1.split()
        words2 = text2.split()
        
        if len(words1) < 3 or len(words2) < 3:
            return 0
        
        # æ£€æŸ¥ç»“å°¾å’Œå¼€å¤´çš„é‡å 
        for i in range(min(10, len(words1))):
            end_words = ' '.join(words1[-(i+1):])
            if text2.startswith(end_words):
                return len(end_words)
        
        return 0
    
    def test_pymupdf_pro_chunking(self, file_path: str) -> Dict[str, Any]:
        """æµ‹è¯•PyMuPDF Proæ–‡ä»¶åˆ‡å—"""
        print(f"æµ‹è¯•æ–‡ä»¶: {file_path}")
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨ PyMuPDF Pro æ‰“å¼€æ–‡æ¡£
            doc = pymupdf.open(file_path)
            print(f"âœ… æˆåŠŸæ‰“å¼€æ–‡æ¡£ï¼Œé¡µæ•°: {len(doc)}")
            
            # æå–æ–‡æœ¬
            text_parts = []
            for page_num, page in enumerate(doc):
                text = page.get_text()
                if text.strip():
                    text_parts.append(f"=== ç¬¬ {page_num + 1} é¡µ ===\n{text}")
            
            full_text = "\n\n".join(text_parts)
            print(f"ğŸ“„ æ–‡æœ¬æå–å®Œæˆï¼Œæ€»å­—ç¬¦æ•°: {len(full_text)}")
            
            # å°è¯•ä½¿ç”¨ PyMuPDF4LLM è¿›è¡Œç»“æ„åŒ–åˆ†å—
            chunks = []
            processing_method = "traditional"
            
            if PYMUPDF4LLM_AVAILABLE:
                try:
                    reader = LlamaMarkdownReader()
                    markdown_text = reader.load_data(file_path)
                    
                    # ä½¿ç”¨ MarkdownHeaderTextSplitter è¿›è¡Œç»“æ„åŒ–åˆ†å—
                    splitter = MarkdownHeaderTextSplitter(
                        headers_to_split_on=[
                            ("#", "æ ‡é¢˜1"),
                            ("##", "æ ‡é¢˜2"),
                            ("###", "æ ‡é¢˜3"),
                        ]
                    )
                    chunks = splitter.split_text(markdown_text)
                    processing_method = "pymupdf4llm"
                    print(f"âœ… PyMuPDF4LLM ç»“æ„åŒ–åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªchunks")
                    
                except Exception as e:
                    print(f"âš ï¸ PyMuPDF4LLM åˆ†å—å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿåˆ†å—: {e}")
                    processing_method = "traditional_fallback"
            
            # å¦‚æœPyMuPDF4LLMä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿåˆ†å—
            if not chunks:
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=DOCUMENT_CONFIG['chunk_size'],
                    chunk_overlap=DOCUMENT_CONFIG['chunk_overlap'],
                    length_function=len,
                )
                chunks = text_splitter.split_text(full_text)
                chunks = [Document(page_content=chunk) for chunk in chunks]
                print(f"âœ… ä¼ ç»Ÿåˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªchunks")
            
            end_time = time.time()
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_text_length = sum(len(chunk.page_content) for chunk in chunks)
            avg_chunk_length = total_text_length / len(chunks) if chunks else 0
            
            # åˆ†æè¿è´¯æ€§
            coherence_analysis = self.analyze_coherence(chunks)
            
            # åˆ†æchunkå†…å®¹
            chunk_analysis = []
            for i, chunk in enumerate(chunks):
                chunk_analysis.append({
                    'chunk_index': i,
                    'length': len(chunk.page_content),
                    'content': chunk.page_content,
                    'coherence_issues': coherence_analysis['issues'] if i < len(coherence_analysis['issues']) else []
                })
            
            return {
                'success': True,
                'processing_time': end_time - start_time,
                'total_chunks': len(chunks),
                'total_text_length': total_text_length,
                'avg_chunk_length': avg_chunk_length,
                'chunk_analysis': chunk_analysis,
                'coherence_analysis': coherence_analysis,
                'file_size_mb': os.path.getsize(file_path) / (1024 * 1024),
                'processing_method': processing_method,
                'pages_count': len(doc)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=" * 60)
        print("PyMuPDF Pro æ–‡ä»¶åˆ‡å—æµ‹è¯•")
        print("=" * 60)
        
        # æµ‹è¯•æ‰€æœ‰æ–‡ä»¶
        for file_type, files in self.test_files.items():
            for file_path in files:
                if os.path.exists(file_path):
                    result = self.test_pymupdf_pro_chunking(file_path)
                    self.results[f"{file_type.upper()}_{Path(file_path).name}"] = result
                    self.print_result(file_type.upper(), Path(file_path).name, result)
                else:
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_report()
    
    def print_result(self, file_type: str, filename: str, result: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print(f"\n{'-' * 50}")
        print(f"æ–‡ä»¶ç±»å‹: {file_type}")
        print(f"æ–‡ä»¶å: {filename}")
        
        if result['success']:
            print(f"âœ… å¤„ç†æˆåŠŸ")
            print(f"ğŸ“Š å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
            print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {result['file_size_mb']:.2f}MB")
            print(f"ğŸ“‘ é¡µæ•°: {result.get('pages_count', 'N/A')}")
            print(f"ğŸ”¢ æ€»å—æ•°: {result['total_chunks']}")
            print(f"ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: {result['total_text_length']}å­—ç¬¦")
            print(f"ğŸ“ å¹³å‡å—é•¿åº¦: {result['avg_chunk_length']:.1f}å­—ç¬¦")
            print(f"ğŸ”§ å¤„ç†æ–¹æ³•: {result.get('processing_method', 'N/A')}")
            
            # è¿è´¯æ€§åˆ†æ
            coherence = result.get('coherence_analysis', {})
            print(f"ğŸ¯ è¿è´¯æ€§è¯„åˆ†: {coherence.get('coherence_score', 0)}/100")
            
            if coherence.get('issues'):
                print(f"\nâš ï¸ è¿è´¯æ€§é—®é¢˜:")
                for issue in coherence['issues']:
                    print(f"  - {issue}")
            
            if coherence.get('strengths'):
                print(f"\nâœ… è¿è´¯æ€§ä¼˜ç‚¹:")
                for strength in coherence['strengths']:
                    print(f"  + {strength}")
            
            # å±•ç¤ºæ‰€æœ‰å—çš„å†…å®¹
            print(f"\nğŸ“‹ æ‰€æœ‰å—å†…å®¹åˆ†æ:")
            for chunk_info in result['chunk_analysis']:
                print(f"\n{'='*30} å— {chunk_info['chunk_index']+1} {'='*30}")
                print(f"é•¿åº¦: {chunk_info['length']} å­—ç¬¦")
                print(f"å†…å®¹:")
                print("-" * 50)
                print(chunk_info['content'])
                print("-" * 50)
        else:
            print(f"âŒ å¤„ç†å¤±è´¥")
            print(f"ğŸš¨ é”™è¯¯ä¿¡æ¯: {result['error']}")
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print(f"\n{'=' * 60}")
        print("PyMuPDF Pro æµ‹è¯•æŠ¥å‘Šæ€»ç»“")
        print(f"{'=' * 60}")
        
        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
        success_count = sum(1 for result in self.results.values() if result['success'])
        total_count = len(self.results)
        
        print(f"ğŸ“ˆ æ€»æµ‹è¯•æ–‡ä»¶æ•°: {total_count}")
        print(f"âœ… æˆåŠŸå¤„ç†: {success_count}")
        print(f"âŒ å¤±è´¥å¤„ç†: {total_count - success_count}")
        
        if success_count > 0:
            # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
            avg_time = sum(result['processing_time'] for result in self.results.values() if result['success']) / success_count
            print(f"â±ï¸ å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ç§’")
            
            # è¿è´¯æ€§ç»Ÿè®¡
            coherence_scores = []
            for result in self.results.values():
                if result['success']:
                    coherence = result.get('coherence_analysis', {})
                    coherence_scores.append(coherence.get('coherence_score', 0))
            
            if coherence_scores:
                avg_coherence = sum(coherence_scores) / len(coherence_scores)
                print(f"ğŸ¯ å¹³å‡è¿è´¯æ€§è¯„åˆ†: {avg_coherence:.1f}/100")
            
            # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
            type_stats = {}
            method_stats = {}
            
            for key, result in self.results.items():
                if result['success']:
                    file_type = key.split('_')[0]
                    if file_type not in type_stats:
                        type_stats[file_type] = []
                    type_stats[file_type].append(result)
                    
                    method = result.get('processing_method', 'unknown')
                    if method not in method_stats:
                        method_stats[method] = 0
                    method_stats[method] += 1
            
            print(f"\nğŸ“Š æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡:")
            for file_type, results in type_stats.items():
                avg_chunks = sum(r['total_chunks'] for r in results) / len(results)
                avg_length = sum(r['avg_chunk_length'] for r in results) / len(results)
                avg_pages = sum(r.get('pages_count', 0) for r in results) / len(results)
                print(f"  {file_type}: å¹³å‡{avg_chunks:.1f}å—, å¹³å‡å—é•¿åº¦{avg_length:.1f}å­—ç¬¦, å¹³å‡{avg_pages:.1f}é¡µ")
            
            print(f"\nğŸ”§ å¤„ç†æ–¹æ³•ç»Ÿè®¡:")
            for method, count in method_stats.items():
                print(f"  {method}: {count}ä¸ªæ–‡ä»¶")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = "pymupdf_pro_chunking_test_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹PyMuPDF Proæ–‡ä»¶åˆ‡å—æµ‹è¯•")
    tester = PyMuPDFProChunkingTester()
    tester.run_all_tests()

if __name__ == "__main__":
    main()
